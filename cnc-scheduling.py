# app.py
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.figure_factory as ff
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
import time
import os
from dotenv import load_dotenv

# Load environment variables (API keys)
load_dotenv()

def trigger_recompute_prompt(ss, label: str):
    """
    Unified helper to handle post-update behavior for:
    - Machine Breakdown
    - Priority Update
    - Outsourcing Policy Update
    """
    # Display user-facing success + guidance
    st.success(f"‚úÖ {label} completed successfully.")
    st.info("üí° Please click **'üß™ Compute All Heuristics'** in the sidebar "
            "to recompute schedules and view updated recommendations.")

    # Set session state to prepare heuristic recomputation
    ss.recalculate_all_heuristics = True
    ss.breakdown_message_visible = True
    ss.current_page = "comparison"

    # Clear Streamlit caches to avoid stale data
    st.cache_data.clear()
    st.cache_resource.clear()

    # Small visual feedback (guarded)
    safe_toast("‚öô Update registered ‚Äî ready for heuristic recomputation.", icon="‚öôÔ∏è")

# ---------------------------
# Helper: make debug visible
# ---------------------------
def dbg(msg):
    # simple debug helper that writes to Streamlit if available
    try:
        st.write(msg)
    except Exception:
        print(msg)


# Guarded toast wrapper: some Streamlit installs may not support st.toast
def safe_toast(message, icon=None):
    try:
        if icon is not None:
            st.toast(message, icon=icon)
        else:
            st.toast(message)
    except Exception:
        try:
            st.success(message)
        except Exception:
            print(message)

# ---------------------------
# Activity Logging System
# ---------------------------
def log_action(ss, action_type, details=None):
    """
    Log user actions for audit trail and activity tracking.
    
    Args:
        ss: Session state
        action_type: Type of action (e.g., 'JOB_ADDED', 'BREAKDOWN_ADDED', 'PRIORITY_CHANGED')
        details: Dictionary with action-specific details
    """
    import datetime
    
    if 'activity_log' not in ss:
        ss.activity_log = []
    
    log_entry = {
        'timestamp': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'action': action_type,
        'details': details or {},
        'user': 'System'  # Can be extended for multi-user support
    }
    
    ss.activity_log.append(log_entry)
    
    # Keep only last 100 entries to avoid memory issues
    if len(ss.activity_log) > 100:
        ss.activity_log = ss.activity_log[-100:]

def get_action_display_text(log_entry):
    """Convert log entry to human-readable text."""
    action = log_entry['action']
    details = log_entry['details']
    timestamp = log_entry['timestamp']
    
    action_messages = {
        'JOB_ADDED': lambda d: f"‚ûï Added job {d.get('job_id', 'N/A')} with {d.get('op_count', 0)} operations (Priority: {d.get('priority', 'N/A')})",
        'JOB_DELETED': lambda d: f"üóëÔ∏è Deleted job {d.get('job_id', 'N/A')} ({d.get('op_count', 0)} operations removed)",
        'BREAKDOWN_ADDED': lambda d: f"üîß Added breakdown to {d.get('machine', 'N/A')} on Day {d.get('day', 'N/A')} ({d.get('duration_min', 0)} min)",
        'BREAKDOWNS_CLEARED': lambda d: f"üßπ Cleared all breakdowns ({d.get('machines_affected', 0)} machines reset)",
        'PRIORITY_CHANGED': lambda d: f"üìä Changed priority for {d.get('job_id', 'N/A')}: {d.get('old_priority', 'N/A')} ‚Üí {d.get('new_priority', 'N/A')}",
        'OUTSOURCING_UPDATED': lambda d: f"üè≠ Updated outsourcing threshold to {d.get('new_threshold', 'N/A')}",
        'HEURISTIC_COMPUTED': lambda d: f"üß™ Computed {d.get('heuristic', 'N/A')} schedule ({d.get('ops_scheduled', 0)} operations)",
        'HEURISTIC_APPLIED': lambda d: f"‚úÖ Applied {d.get('heuristic', 'N/A')} schedule to dataset",
        'DATA_LOADED': lambda d: f"üìÇ Loaded dataset ({d.get('total_ops', 0)} operations, {d.get('total_jobs', 0)} jobs)",
    }
    
    message_func = action_messages.get(action, lambda d: f"‚Ä¢ {action}: {d}")
    message = message_func(details)
    
    return f"**{timestamp}** - {message}"

def draw_activity_log(ss):
    """Display activity log in a dedicated section."""
    import datetime
    
    st.subheader("üìú Activity Log")
    
    if 'activity_log' not in ss or len(ss.activity_log) == 0:
        st.info("No activities logged yet. Actions like adding jobs, breakdowns, or changing priorities will appear here.")
        return
    
    # Filter options
    col1, col2, col3 = st.columns(3)
    with col1:
        show_count = st.selectbox("Show last:", [10, 25, 50, 100], index=1, key='log_show_count')
    with col2:
        filter_type = st.selectbox("Filter by:", ['All', 'Jobs', 'Breakdowns', 'Priority', 'Heuristics'], key='log_filter')
    with col3:
        if st.button("Clear Log", key='clear_log'):
            ss.activity_log = []
            st.rerun()
    
    # Filter logs
    filtered_logs = ss.activity_log.copy()
    
    if filter_type == 'Jobs':
        filtered_logs = [log for log in filtered_logs if 'JOB' in log['action']]
    elif filter_type == 'Breakdowns':
        filtered_logs = [log for log in filtered_logs if 'BREAKDOWN' in log['action']]
    elif filter_type == 'Priority':
        filtered_logs = [log for log in filtered_logs if 'PRIORITY' in log['action']]
    elif filter_type == 'Heuristics':
        filtered_logs = [log for log in filtered_logs if 'HEURISTIC' in log['action']]
    
    # Show latest entries (reversed for newest first)
    display_logs = list(reversed(filtered_logs[-show_count:]))
    
    if not display_logs:
        st.info(f"No {filter_type.lower()} activities found.")
        return
    
    # Display logs
    st.caption(f"Showing {len(display_logs)} of {len(ss.activity_log)} total entries")
    
    for log_entry in display_logs:
        action_type = log_entry['action']
        
        # Color code by action type
        if 'ADDED' in action_type or 'COMPUTED' in action_type:
            st.success(get_action_display_text(log_entry), icon="‚úÖ")
        elif 'DELETED' in action_type or 'CLEARED' in action_type:
            st.warning(get_action_display_text(log_entry), icon="üóëÔ∏è")
        elif 'CHANGED' in action_type or 'UPDATED' in action_type:
            st.info(get_action_display_text(log_entry), icon="üîÑ")
        else:
            st.markdown(get_action_display_text(log_entry))
    
    # Export option
    if st.button("üì• Export Log as CSV", key='export_log'):
        import pandas as pd
        log_df = pd.DataFrame(ss.activity_log)
        csv = log_df.to_csv(index=False)
        st.download_button(
            label="Download CSV",
            data=csv,
            file_name=f"activity_log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )

# ---------------------------
# AI Helper Functions (Gemini)
# ---------------------------
def get_gemini_model():
    """Initialize and return Gemini AI model if API key is available."""
    try:
        import google.generativeai as genai
        api_key = os.getenv('GEMINI_API_KEY')
        if api_key:
            from google.generativeai import GenerativeModel
            genai.configure(api_key=api_key)
            # Use gemini-1.5-flash (latest free model) instead of deprecated gemini-pro
            return GenerativeModel("gemini-flash-latest")
        return None
    except ImportError:
        return None
    except Exception as e:
        st.warning(f"Gemini AI initialization failed: {str(e)}")
        return None

def generate_ai_insights(prompt, context_data=None):
    """Generate AI insights using Gemini."""
    model = get_gemini_model()
    if not model:
        return None
    
    try:
        full_prompt = prompt
        if context_data:
            full_prompt = f"{context_data}\n\n{prompt}"
        
        response = model.generate_content(full_prompt)
        return response.text
    except Exception as e:
        st.error(f"AI generation failed: {str(e)}")
        return None

# ---------------------------
# Old helpers (from your code)
# ---------------------------
def parse_maintenance(maintenance_str):
    if pd.isna(maintenance_str) or maintenance_str == 'None':
        return None
    try:
        parts = maintenance_str.replace("Day", "").replace(",", "").strip().split()
        day = int(parts[0])
        times = parts[1].split('-')
        start_hour, start_min = map(int, times[0].split(':'))
        end_hour, end_min = map(int, times[1].split(':'))
        MINUTES_PER_WORKDAY = 8 * 60
        WORK_START_HOUR = 8
        WORK_END_HOUR = 16

        def clock_to_work_minutes(hour, minute):
            if WORK_START_HOUR <= hour < WORK_END_HOUR:
                work_hour = hour - WORK_START_HOUR
                return work_hour * 60 + minute
            else:
                return None

        start_work_min = clock_to_work_minutes(start_hour, start_min)
        end_work_min = clock_to_work_minutes(end_hour, end_min)

        if start_work_min is None or end_work_min is None:
            return None

        start_time = (day - 1) * MINUTES_PER_WORKDAY + start_work_min
        end_time = (day - 1) * MINUTES_PER_WORKDAY + end_work_min

        return {'start': start_time, 'end': end_time, 'duration': end_time - start_time}
    except Exception:
        return None

def get_eligible_machines(op_type):
    if op_type == 'MILLING':
        return ['M1', 'M3', 'M4']
    elif op_type == 'TURNING':
        return ['M6', 'M9']
    elif op_type == 'GRINDING':
        return ['M6', 'M9']
    elif op_type == 'DRILLING':
        return ['M1', 'M3', 'M4']
    else:
        return []

def calculate_inhouse_cost(operation, df_effective, hourly_rate=50):
    op_id = operation['Operation_ID']
    eligible = df_effective[df_effective['Operation_ID'] == op_id]

    if len(eligible) == 0:
        return float('inf'), None

    best_option = eligible.loc[eligible['Total_Time'].idxmin()]
    labor_cost = (best_option['Total_Time'] / 60) * hourly_rate
    material_cost = operation['Quantity'] * 0.5
    total_cost = labor_cost + material_cost

    return total_cost, best_option['Machine_ID']

def make_or_buy_decision(operation, df_effective, cost_threshold=0.9, hourly_rate=50):
    inhouse_cost, best_machine = calculate_inhouse_cost(operation, df_effective, hourly_rate)
    inhouse_time = operation.get('Total_Proc_Min', operation.get('Proc_Time_per_Unit', 0) * operation.get('Quantity', 1)) + operation.get('Setup_Time', 0)
    outsource_cost = operation.get('Outsource_Cost', np.inf)
    outsource_time = operation.get('Outsource_Time_Min', np.inf)

    earliest_start = operation.get('Release_Time_Min', 0)
    earliest_finish = earliest_start + inhouse_time
    can_meet_deadline = earliest_finish <= operation.get('Due_Time_Min', np.inf)

    if not can_meet_deadline and outsource_time < inhouse_time:
        return 'OUTSOURCE', outsource_cost, 'Deadline constraint'
    if outsource_cost < (inhouse_cost * cost_threshold):
        return 'OUTSOURCE', outsource_cost, 'Cost advantage'
    return 'IN_HOUSE', inhouse_cost, 'Best in-house'

def get_setup_penalty(prev_material, next_material, df_penalties):
    if not prev_material or not next_material:
        return 0
    penalty = df_penalties[
        (df_penalties['Previous Material'] == prev_material) &
        (df_penalties['Next Material'] == next_material)
    ]
    return penalty.iloc[0]['Penalty Time (min)'] if len(penalty) > 0 else 15

# ---------------------------
# Metrics (unchanged logic)
# ---------------------------

def calculate_metrics(schedule_df, df_ops, heuristic_name):
    if schedule_df is None or schedule_df.empty:
        return {
            'Heuristic': heuristic_name,
            'Makespan_Days': 0,
            'Total_Tardiness_Days': 0,
            'Total_Cost_$': 0,
            'On_Time_%': 0,
            'Machine_Utilization_%': 0
        }

    makespan_min = schedule_df['End_Time'].max() if not schedule_df.empty else 0
    makespan_days = makespan_min / 480

    total_tardiness_min = schedule_df['Tardiness'].sum() if 'Tardiness' in schedule_df.columns else 0
    total_tardiness_days = total_tardiness_min / 480

    late_ops = len(schedule_df[schedule_df['Tardiness'] > 0]['Operation_ID'].unique()) if not schedule_df.empty else 0
    total_ops = df_ops['Operation_ID'].nunique()

    if late_ops > total_ops:
        late_ops = total_ops

    avg_tardiness = schedule_df.groupby('Operation_ID')['Tardiness'].sum().mean() if not schedule_df.empty else 0

    if 'base_df_machines' in st.session_state:
        machine_count = len(st.session_state.base_df_machines)
    else:
        machine_count = 5

    total_setup_time = schedule_df['Setup_Time'].sum() if 'Setup_Time' in schedule_df.columns else 0
    total_proc_time = schedule_df['Proc_Time'].sum() if 'Proc_Time' in schedule_df.columns else 0
    total_transfer_time = schedule_df['Transfer_Time'].sum() if 'Transfer_Time' in schedule_df.columns else 0
    total_productive_time = total_setup_time + total_proc_time + total_transfer_time

    total_available_time = machine_count * makespan_min if makespan_min > 0 else machine_count * 1
    utilization = (total_productive_time / total_available_time) * 100 if total_available_time > 0 else 0

    machine_utilization = {}
    if not schedule_df.empty:
        for machine_id in schedule_df['Machine_ID'].unique():
            machine_ops = schedule_df[schedule_df['Machine_ID'] == machine_id]
            machine_productive = (machine_ops['Setup_Time'].sum() +
                                machine_ops['Proc_Time'].sum() +
                                machine_ops['Transfer_Time'].sum())
            machine_util = (machine_productive / makespan_min) * 100 if makespan_min > 0 else 0
            machine_utilization[machine_id] = round(machine_util, 1)

    inhouse_cost = schedule_df['Proc_Time'].sum() / 60 * 50 if 'Proc_Time' in schedule_df.columns else 0
    outsource_cost = df_ops[df_ops['Assignment_Type'] == 'OUTSOURCE']['Outsource_Cost'].sum() if 'Outsource_Cost' in df_ops.columns else 0
    total_cost = inhouse_cost + outsource_cost

    ontime_pct = ((total_ops - late_ops) / total_ops) * 100 if total_ops > 0 else 100
    ontime_pct = max(0, min(100, ontime_pct))

    return {
        'Heuristic': heuristic_name,
        'Makespan_Days': round(makespan_days, 2),
        'Total_Tardiness_Days': round(total_tardiness_days, 2),
        'Late_Operations': int(late_ops),
        'Total_Operations': int(total_ops),
        'On_Time_%': round(ontime_pct, 1),
        'Avg_Tardiness_Min': round(avg_tardiness, 1),
        'Machine_Utilization_%': round(utilization, 1),
        'Total_Cost_$': round(total_cost, 2),
        '_Machine_Details': machine_utilization,
        '_Scheduled_Ops': len(schedule_df),
        '_Total_Ops': len(df_ops),
        '_Productive_Time_Days': round(total_productive_time / 480, 2)
    }

def refresh_all_heuristics_metrics(ss):
    heuristics = ['SPT', 'EDD', 'CR', 'PRIORITY', 'BALANCED', 'DEADLINE_FIRST']
    metrics = []

    for heur in heuristics:
        schedule_key = f'schedule_{heur.lower()}'
        schedule = getattr(ss, schedule_key, None)

        if schedule is None or ss.get('force_metric_refresh', False):
            scheduler = CNCScheduler(
                ss.base_df_ops,
                ss.base_df_machines,
                ss.base_df_effective,
                ss.base_df_penalties
            )
            schedule = scheduler.run_scheduling(heuristic=heur)
            setattr(ss, schedule_key, schedule)

        metrics.append(calculate_metrics(schedule, ss.base_df_ops, heur))

    ss.df_metrics = pd.DataFrame(metrics)
    ss.force_metric_refresh = False

# ---------------------------
# CNCScheduler (unchanged)
# ---------------------------
class CNCScheduler:
    def __init__(self, df_ops, df_machines, df_effective, df_penalties):
        self.df_ops = df_ops.copy()
        self.df_machines = df_machines.copy()
        self.df_effective = df_effective.copy()
        self.df_penalties = df_penalties

        self.machine_availability = {m: 0 for m in df_machines['Machine_ID']}
        self.machine_last_material = {m: None for m in df_machines['Machine_ID']}

        self.schedule = []
        self.op_completion_times = {}

    def reset(self):
        self.machine_availability = {m: 0 for m in self.df_machines['Machine_ID']}
        self.machine_last_material = {m: None for m in self.df_machines['Machine_ID']}
        self.schedule = []
        self.op_completion_times = {}

    def get_earliest_available_time(self, machine_id, release_time, duration):
        """
        Determine the earliest available start time for a machine, considering
        current availability, job release time, and any maintenance/breakdown windows.
        """
        # Start from max of current availability and job release
        current_avail = max(self.machine_availability.get(machine_id, 0), release_time)

        # Try fetching machine info safely
        try:
            machine_row = self.df_machines[self.df_machines['Machine_ID'] == machine_id]
            if machine_row.empty:
                return current_avail
            maintenance = machine_row.iloc[0].get('Maintenance_Window', None)
        except Exception:
            return current_avail

        # ‚úÖ Handle no maintenance/breakdown case
        if maintenance is None or (isinstance(maintenance, dict) and not maintenance):
            return current_avail

        # ‚úÖ Normalize to a list for multiple windows
        maintenance_list = (
            [maintenance] if isinstance(maintenance, dict)
            else [m for m in maintenance if isinstance(m, dict)]
        )

        # Sort maintenance windows by start time for safety
        maintenance_list.sort(key=lambda mw: mw.get('start', 0))

        # ‚úÖ Adjust for any overlap between current availability and maintenance windows
        while True:
            adjusted = False
            for mw in maintenance_list:
                mw_start = mw.get('start', 0)
                mw_end = mw.get('end', 0)

                # Skip invalid windows
                if mw_end <= mw_start:
                    continue

                end_time = current_avail + duration

                # Check if this job would overlap the maintenance window
                overlap = (current_avail < mw_end) and (end_time > mw_start)
                if overlap:
                    # Move start to after maintenance end
                    current_avail = mw_end
                    adjusted = True
                    break  # Recheck all windows again
            if not adjusted:
                break

        # ‚úÖ Update machine availability to reflect the new time slot
        self.machine_availability[machine_id] = current_avail + duration

        return current_avail


    def get_available_operations(self):
        available = []
        for idx, op in self.df_ops.iterrows():
            op_id = op['Operation_ID']
            if op.get('Assignment_Type', 'IN_HOUSE') == 'OUTSOURCE' or op_id in self.op_completion_times:
                continue

            same_job = self.df_ops[self.df_ops['Job_ID'] == op['Job_ID']].sort_values('Op_Seq')
            all_pred_done = True
            earliest_start = op.get('Release_Time_Min', 0)

            for _, pred in same_job.iterrows():
                if pred['Op_Seq'] < op['Op_Seq']:
                    if pred['Operation_ID'] not in self.op_completion_times:
                        if pred.get('Assignment_Type', 'IN_HOUSE') == 'OUTSOURCE':
                            outsource_complete_time = pred.get('Release_Time_Min', 0) + pred.get('Outsource_Time_Min', 0)
                            earliest_start = max(earliest_start, outsource_complete_time)
                            self.op_completion_times[pred['Operation_ID']] = outsource_complete_time
                        else:
                            all_pred_done = False
                            break
                    else:
                        earliest_start = max(earliest_start, self.op_completion_times[pred['Operation_ID']])
            if all_pred_done:
                available.append((op, earliest_start))
        return available

    def find_best_machine(self, operation, earliest_start_time):
        op_id = operation['Operation_ID']
        eligible = self.df_effective[self.df_effective['Operation_ID'] == op_id]
        if len(eligible) == 0:
            return None, float('inf')

        best_machine = None
        best_completion = float('inf')
        for _, machine_option in eligible.iterrows():
            machine_id = machine_option['Machine_ID']
            eff_time = machine_option['Effective_Proc_Time']

            prev_material = self.machine_last_material.get(machine_id)
            setup_penalty = get_setup_penalty(prev_material, operation.get('Mat_Type', None), self.df_penalties)

            actual_setup_time = operation.get('Setup_Time', 0) + setup_penalty
            transfer_time = operation.get('Transfer_Min', 0)
            total_duration = actual_setup_time + eff_time + transfer_time

            start_time = self.get_earliest_available_time(machine_id, earliest_start_time, total_duration)
            completion_time = start_time + total_duration

            if completion_time < best_completion:
                best_completion = completion_time
                best_machine = machine_id
        return best_machine, best_completion

    def schedule_operation(self, operation, machine_id, earliest_start_time):
        op_id = operation['Operation_ID']
        op_details_query = self.df_effective[
            (self.df_effective['Operation_ID'] == op_id) &
            (self.df_effective['Machine_ID'] == machine_id)
        ]
        if len(op_details_query) == 0:
            return False

        op_details = op_details_query.iloc[0]
        eff_time = op_details['Effective_Proc_Time']

        prev_material = self.machine_last_material.get(machine_id)
        setup_penalty = get_setup_penalty(prev_material, operation.get('Mat_Type', None), self.df_penalties)
        actual_setup_time = operation.get('Setup_Time', 0) + setup_penalty

        transfer_time = operation.get('Transfer_Min', 0)
        total_duration = actual_setup_time + eff_time + transfer_time

        start_time = self.get_earliest_available_time(machine_id, earliest_start_time, total_duration)
        end_time = start_time + total_duration

        self.schedule.append({
            'Operation_ID': op_id,
            'Job_ID': operation['Job_ID'],
            'Machine_ID': machine_id,
            'Start_Time': start_time,
            'End_Time': end_time,
            'Setup_Time': actual_setup_time,
            'Proc_Time': eff_time,
            'Transfer_Time': transfer_time,
            'Due_Time': operation.get('Due_Time_Min', 0),
            'Tardiness': max(0, end_time - operation.get('Due_Time_Min', 0))
        })

        self.machine_availability[machine_id] = end_time
        self.machine_last_material[machine_id] = operation.get('Mat_Type', None)
        self.op_completion_times[op_id] = end_time
        return True

    def select_next_operation(self, available_ops, heuristic='SPT'):
        def safe_priority(op):
            return int(op.get('Priority', 3))
        
        def calculate_slack(op):
            """Calculate slack time (Due - Current Time - Processing Time)"""
            current_time = min(self.machine_availability.values())
            slack = op['Due_Time_Min'] - current_time - op['Total_Proc_Min']
            return max(slack, 0.1)  # Avoid division by zero
        
        def calculate_weighted_score(op):
            """Balanced scoring combining multiple factors"""
            # Normalize factors (lower is better for all)
            priority_score = safe_priority(op) / 4.0  # Normalize 1-4 to 0.25-1.0
            time_score = op['Total_Proc_Min'] / 500.0  # Normalize typical range
            slack_score = calculate_slack(op) / 1000.0  # Normalize slack
            
            # Weighted combination (adjust weights based on business needs)
            score = (0.4 * priority_score +  # 40% priority
                    0.3 * slack_score +       # 30% urgency (slack)
                    0.3 * time_score)         # 30% processing time
            return score

        if heuristic == 'SPT':
            rule = "SPT (Shortest Processing Time)"
        elif heuristic == 'EDD':
            rule = "EDD (Earliest Due Date)"
        elif heuristic == 'CR':
            rule = "CR (Critical Ratio)"
        elif heuristic == 'PRIORITY':
            rule = "PRIORITY (Priority-Driven)"
        elif heuristic == 'BALANCED':
            rule = "BALANCED (Multi-Factor Weighted)"
        elif heuristic == 'DEADLINE_FIRST':
            rule = "DEADLINE_FIRST (Urgency-Focused)"
        else:
            rule = "SPT (Default)"

        st.caption(f"‚öôÔ∏è Active Selection Rule: {rule}")

        if heuristic == 'SPT':
            op, earliest_start = min(
                available_ops,
                key=lambda x: (safe_priority(x[0]), x[0]['Total_Proc_Min'], x[0]['Due_Time_Min'])
            )
        elif heuristic == 'EDD':
            op, earliest_start = min(
                available_ops,
                key=lambda x: (safe_priority(x[0]), x[0]['Due_Time_Min'], x[0]['Total_Proc_Min'])
            )
        elif heuristic == 'CR':
            op, earliest_start = min(
                available_ops,
                key=lambda x: (safe_priority(x[0]), (x[0]['Due_Time_Min'] / max(x[0]['Total_Proc_Min'], 1)))
            )
        elif heuristic == 'PRIORITY':
            op, earliest_start = min(
                available_ops,
                key=lambda x: (safe_priority(x[0]), x[0]['Due_Time_Min'])
            )
        elif heuristic == 'BALANCED':
            # New: Multi-factor weighted scoring
            op, earliest_start = min(
                available_ops,
                key=lambda x: calculate_weighted_score(x[0])
            )
        elif heuristic == 'DEADLINE_FIRST':
            # New: Focus on preventing deadline misses
            op, earliest_start = min(
                available_ops,
                key=lambda x: (safe_priority(x[0]), calculate_slack(x[0]), x[0]['Total_Proc_Min'])
            )
        else:
            op, earliest_start = min(
                available_ops,
                key=lambda x: (safe_priority(x[0]), x[0]['Total_Proc_Min'])
            )

        return op, earliest_start

    def run_scheduling(self, heuristic='SPT', verbose=True):
        if verbose:
            st.write(f"üîÑ Starting {heuristic} scheduling...")

        self.reset()
        outsourced_ops = self.df_ops[self.df_ops.get('Assignment_Type', 'IN_HOUSE') == 'OUTSOURCE']
        if verbose:
            st.write(f"  ‚úì Outsourced: {len(outsourced_ops)} operations (auto-complete)")

        for _, op in outsourced_ops.iterrows():
            self.op_completion_times[op['Operation_ID']] = op.get('Release_Time_Min', 0) + op.get('Outsource_Time_Min', 0)

        non_outsourced = self.df_ops[self.df_ops.get('Assignment_Type', 'IN_HOUSE') != 'OUTSOURCE']
        operations_count = len(non_outsourced)
        scheduled_ops_set = set()

        if verbose:
            st.write(f"  ‚úì Scheduling in-house: {operations_count} operations")

        max_iterations = operations_count * 2 if operations_count > 0 else 1000
        iteration = 0

        chain = {
            'SPT': "SPT",
            'EDD': "EDD",
            'CR': "CR",
            'PRIORITY': "PRIORITY"
        }.get(heuristic, "SPT")
        st.info(f"üß© Using Scheduling Rule: **{chain}**")

        while len(scheduled_ops_set) < operations_count:
            iteration += 1
            if iteration > max_iterations:
                st.warning(f"‚ö†Ô∏è Max iterations reached ({iteration})")
                break

            available = self.get_available_operations()
            available = [op for op in available if op[0]['Operation_ID'] not in scheduled_ops_set]

            if not available:
                st.write("‚ö†Ô∏è No available operations remaining.")
                break

            next_op, earliest_start_time = self.select_next_operation(available, heuristic=heuristic)
            if next_op is None:
                st.write("‚ö†Ô∏è No operation selected ‚Äî stopping.")
                break

            best_machine, best_completion = self.find_best_machine(next_op, earliest_start_time)
            if best_machine is None:
                self.op_completion_times[next_op['Operation_ID']] = float('inf')
                scheduled_ops_set.add(next_op['Operation_ID'])
                continue

            success = self.schedule_operation(next_op, best_machine, earliest_start_time)
            if success:
                scheduled_ops_set.add(next_op['Operation_ID'])

            if verbose and iteration % 100 == 0:
                st.write(f"‚è≥ Progress: {len(scheduled_ops_set)}/{operations_count} scheduled")

        if verbose:
            st.success(f"‚úÖ {heuristic} scheduling complete: {len(scheduled_ops_set)}/{operations_count} operations scheduled")

        st.session_state.triggered_by_priority_manager = False
        return pd.DataFrame(self.schedule)

# ---------------------------
# Live capacity & data loading
# ---------------------------
def analyze_capacity_for_new_job(new_job_ops, current_schedule, df_machines, df_effective, due_time_min):
    analysis = {
        'feasible': False,
        'recommendation': 'OUTSOURCE',
        'reasons': [],
        'metrics': {}
    }

    if not current_schedule.empty:
        current_makespan = current_schedule['End_Time'].max()
        total_machines = len(df_machines)
        total_productive = (current_schedule['Setup_Time'].sum() +
                           current_schedule['Proc_Time'].sum() +
                           current_schedule['Transfer_Time'].sum())
        avg_utilization = (total_productive / (current_makespan * total_machines)) * 100 if current_makespan > 0 else 0
        analysis['metrics']['current_makespan_days'] = current_makespan / 480
        analysis['metrics']['current_utilization'] = round(avg_utilization, 1)
    else:
        current_makespan = 0
        analysis['metrics']['current_makespan_days'] = 0
        analysis['metrics']['current_utilization'] = 0

    total_new_time = 0
    operations_schedulable = 0

    for op in new_job_ops:
        op_id = op['Operation_ID']
        eligible_times = df_effective[df_effective['Operation_ID'] == op_id]
        if not eligible_times.empty:
            min_time = eligible_times['Total_Time'].min()
            total_new_time += min_time
            operations_schedulable += 1

    if operations_schedulable == 0:
        analysis['feasible'] = False
        analysis['recommendation'] = 'OUTSOURCE'
        analysis['reasons'].append("‚ùå No eligible machines found for operations")
        return analysis

    if operations_schedulable < len(new_job_ops):
        analysis['reasons'].append(f"‚ö†Ô∏è Only {operations_schedulable}/{len(new_job_ops)} operations can be scheduled in-house")

    estimated_completion = current_makespan + total_new_time
    analysis['metrics']['estimated_completion_days'] = estimated_completion / 480
    analysis['metrics']['due_date_days'] = due_time_min / 480
    analysis['metrics']['new_job_time_days'] = total_new_time / 480

    deadline_buffer = due_time_min - estimated_completion
    analysis['metrics']['deadline_buffer_days'] = deadline_buffer / 480

    if deadline_buffer < 0:
        analysis['feasible'] = False
        analysis['recommendation'] = 'OUTSOURCE'
        analysis['reasons'].append(f"‚ùå Cannot meet deadline - Need {abs(deadline_buffer)/480:.1f} more days")
        analysis['reasons'].append(f"   Estimated completion: Day {estimated_completion/480:.1f}")
        analysis['reasons'].append(f"   Due date: Day {due_time_min/480:.1f}")
    else:
        analysis['feasible'] = True
        analysis['recommendation'] = 'SCHEDULE'
        analysis['reasons'].append(f"‚úÖ Can meet deadline with {deadline_buffer/480:.1f} days buffer")

    if current_makespan > 0:
        projected_utilization = ((total_productive + total_new_time) / (estimated_completion * len(df_machines))) * 100 if estimated_completion > 0 else 0
        analysis['metrics']['projected_utilization'] = round(projected_utilization, 1)

        if projected_utilization > 90:
            analysis['reasons'].append(f"‚ö†Ô∏è High utilization: {projected_utilization:.1f}% (machines heavily loaded)")
        elif projected_utilization > 75:
            analysis['reasons'].append(f"‚úÖ Good utilization: {projected_utilization:.1f}% (balanced load)")
        else:
            analysis['reasons'].append(f"‚úÖ Low utilization: {projected_utilization:.1f}% (capacity available)")

    return analysis

# ---------------------------
# Data loading & preprocessing
# ---------------------------
@st.cache_data
def load_all_data(sample_size=None, _cache_version=2):
    try:
        df_ops = pd.read_csv("data/jobs_dataset.csv")
        df_vendors = pd.read_csv("data/vendor_data.csv")
        df_machines = pd.read_csv("data/machine_data.csv")
        df_penalties = pd.read_csv("data/previous_next_material.csv")
    except FileNotFoundError as e:
        st.error(f"Error loading data: {e}. Make sure the 'data' folder is in the same directory as app.py")
        st.stop()

    st.write("üîç DEBUG: Validating data quality...")
    validation_issues = []

    required_ops_cols = ['Job_ID', 'Operation_ID', 'Op_Seq', 'Quantity', 'Op_Type',
                         'Mat_Type', 'Proc_Time_per_Unit', 'Setup_Time', 'Release_Day', 'Due_Day']
    missing_cols = [col for col in required_ops_cols if col not in df_ops.columns]
    if missing_cols:
        st.error(f"‚ùå Missing required columns in jobs_dataset.csv: {missing_cols}")
        st.stop()

    if (df_ops['Quantity'] <= 0).any():
        validation_issues.append(f"Found {(df_ops['Quantity'] <= 0).sum()} operations with Quantity <= 0")
    if (df_ops['Proc_Time_per_Unit'] < 0).any():
        validation_issues.append(f"Found {(df_ops['Proc_Time_per_Unit'] < 0).sum()} operations with negative processing time")
    if (df_ops['Setup_Time'] < 0).any():
        validation_issues.append(f"Found {(df_ops['Setup_Time'] < 0).sum()} operations with negative setup time")

    if validation_issues:
        st.warning("‚ö†Ô∏è Data Quality Issues Found:")
        for issue in validation_issues:
            st.write(f"   - {issue}")
    else:
        st.success("‚úÖ Data validation passed")

    if sample_size:
        unique_jobs = df_ops['Job_ID'].unique()[:sample_size]
        df_ops = df_ops[df_ops['Job_ID'].isin(unique_jobs)].copy()
        st.info(f"üìä **TEST MODE**: Using {sample_size} jobs ({len(df_ops)} operations) for faster performance")

    st.write("üîç DEBUG: Checking for deadline anomalies...")
    deadline_issues = df_ops[df_ops['Due_Day'] <= df_ops['Release_Day']]
    if len(deadline_issues) > 0:
        st.warning(f"‚ö†Ô∏è Found {len(deadline_issues)} operations with impossible deadlines!")
        for idx in deadline_issues.index:
            release_day = df_ops.at[idx, 'Release_Day']
            proc_time_days = df_ops.at[idx, 'Proc_Time_per_Unit'] * df_ops.at[idx, 'Quantity'] / 480
            setup_time_days = df_ops.at[idx, 'Setup_Time'] / 480
            lead_time = max(7, int(proc_time_days + setup_time_days + 3))
            df_ops.at[idx, 'Due_Day'] = release_day + lead_time
        st.success(f"‚úÖ Fixed {len(deadline_issues)} deadline issues - added realistic lead times")
    else:
        st.success("‚úÖ No deadline anomalies found")

    df_ops['Total_Proc_Min'] = df_ops['Proc_Time_per_Unit'] * df_ops['Quantity']

    # Normalize machines df columns
    if 'Speed Factor' not in df_machines.columns and 'SpeedFactor' in df_machines.columns:
        df_machines.rename(columns={'SpeedFactor': 'Speed Factor'}, inplace=True)

    df_machines['Speed Factor'] = (
        df_machines['Speed Factor']
        .astype(str)
        .str.extract(r'([0-9]*\.?[0-9]+)')
        .astype(float)
    )

    effective_times = []
    for idx, op in df_ops.iterrows():
        eligible_machines = get_eligible_machines(op['Op_Type'])
        if not eligible_machines:
            continue

        total_proc_min = float(op['Proc_Time_per_Unit']) * float(op['Quantity'])
        setup_time = float(op['Setup_Time'])
        transfer_min = float(op.get('Transfer_Min', 0))

        for machine_id in eligible_machines:
            machine = df_machines[df_machines['Machine ID'] == machine_id].iloc[0]
            speed_factor = float(machine['Speed Factor'])
            oee = float(machine['OEE (Uptime)'])
            effective_time = total_proc_min * speed_factor * (1 / oee)
            total_time = effective_time + setup_time + transfer_min

            effective_times.append({
                'Operation_ID': op['Operation_ID'],
                'Machine_ID': machine_id,
                'Effective_Proc_Time': effective_time,
                'Setup_Time': setup_time,
                'Transfer_Min': transfer_min,
                'Total_Time': total_time
            })
    df_effective = pd.DataFrame(effective_times)

    df_vendors['Outsource_Unit_Cost'] = df_vendors['Outsource_Unit_Cost'].replace('[\\$,]', '', regex=True).astype(float)
    df_vendors['Transport_Cost'] = df_vendors['Transport_Cost'].replace('[\\$,]', '', regex=True).astype(float)

    df_ops_vendor = df_ops.merge(
        df_vendors[['Vendor_ID', 'Outsource_Lead_Time (Days)', 'Outsource_Unit_Cost', 'Transport_Cost', 'Quality_Factor']],
        left_on='Vendor_Ref', right_on='Vendor_ID', how='left'
    )

    df_ops_vendor['Outsource_Cost'] = (
        (df_ops_vendor['Outsource_Unit_Cost'] * df_ops_vendor['Quantity']) + df_ops_vendor['Transport_Cost']
    ) / df_ops_vendor['Quality_Factor']

    df_ops_vendor['Outsource_Time_Min'] = df_ops_vendor['Outsource_Lead_Time (Days)'] * 8 * 60

    df_ops = df_ops.merge(
        df_ops_vendor[['Operation_ID', 'Outsource_Cost', 'Outsource_Time_Min']],
        on='Operation_ID', how='left'
    )

    MINUTES_PER_DAY = 8 * 60
    df_ops['Release_Time_Min'] = df_ops['Release_Day'] * MINUTES_PER_DAY
    df_ops['Due_Time_Min'] = df_ops['Due_Day'] * MINUTES_PER_DAY
    df_ops['Outsource_Cost'].fillna(0, inplace=True)
    df_ops['Outsource_Time_Min'].fillna(0, inplace=True)
    df_ops['Completion_Day'] = 0

    df_machines = df_machines.rename(columns={'Machine ID': 'Machine_ID'})
    df_machines['Maintenance_Window'] = df_machines['Scheduled Maintenance (Day, Time-Time)'].apply(parse_maintenance)

    # Use session state cost threshold if available, otherwise default to 0.9
    active_cost_threshold = st.session_state.get('cost_threshold', 0.9)
    
    decisions = []
    for idx, op in df_ops.iterrows():
        decision, cost, reason = make_or_buy_decision(op, df_effective, cost_threshold=active_cost_threshold)
        decisions.append({'Operation_ID': op['Operation_ID'], 'Decision': decision, 'Reason': reason, 'Cost': cost})

    df_decisions = pd.DataFrame(decisions)
    df_ops = df_ops.merge(df_decisions, on='Operation_ID', how='left')
    df_ops['Assignment_Type'] = df_ops['Decision'].fillna('IN_HOUSE')
    df_ops.drop(columns=['Decision'], inplace=True)

    total_ops = len(df_ops)
    outsourced_ops = len(df_ops[df_ops['Assignment_Type'] == 'OUTSOURCE'])
    inhouse_ops = len(df_ops[df_ops['Assignment_Type'] == 'IN_HOUSE'])
    outsource_pct = (outsourced_ops / total_ops) * 100 if total_ops > 0 else 0

    st.write(f"üìä **Make-or-Buy Analysis:**")
    st.write(f"   - Total Operations: {total_ops}")
    st.write(f"   - In-House: {inhouse_ops} ({100-outsource_pct:.1f}%)")
    st.write(f"   - Outsourced: {outsourced_ops} ({outsource_pct:.1f}%)")

    if outsource_pct > 50:
        st.warning(f"‚ö†Ô∏è **HIGH OUTSOURCING**: {outsource_pct:.0f}% of operations outsourced! This will reduce machine utilization.")
        st.write("   üí° Consider: Lowering cost threshold or adding machine capacity")

    return df_ops, df_machines, df_effective, df_penalties, df_vendors


def run_single_heuristic(_df_ops, _df_machines, _df_effective, _df_penalties, heuristic='SPT'):
    st.write(f"üîç **EXPLAINER**: Running {heuristic} scheduling algorithm")
    st.write(f"   üìä Processing {len(_df_ops)} operations across {len(_df_machines)} machines")

    import time
    start_time = time.time()

    scheduler = CNCScheduler(_df_ops, _df_machines, _df_effective, _df_penalties)
    init_time = time.time() - start_time
    st.write(f"   ‚úÖ Scheduler initialized in {init_time:.2f}s")

    st.write(f"   üîÑ Executing {heuristic} heuristic (this may take some time)...")
    heuristic_start = time.time()
    schedule = scheduler.run_scheduling(heuristic=heuristic)
    exec_time = time.time() - heuristic_start

    st.write(f"   ‚úÖ {heuristic} completed in {exec_time:.2f}s")
    st.write(f"   üìà Successfully scheduled {len(schedule)} operations")

    return schedule

# ---------------------------
# Visualization helpers (unchanged but included)
# ---------------------------

def create_gantt_chart(
    _schedule_df,
    _machines_df,
    title="CNC Machine Schedule",
    _cache_key="",
    machines_order=None
):
    import re
    import pandas as pd
    import plotly.graph_objects as go

    schedule_df = _schedule_df.copy()
    machines_df = _machines_df.copy()

    for col in ["Start_Time", "End_Time"]:
        if col in schedule_df.columns:
            schedule_df[col] = pd.to_numeric(schedule_df[col], errors="coerce").fillna(0)

    def machine_sort_key(mid):
        match = re.search(r"\d+", str(mid))
        return int(match.group()) if match else float("inf")

    if machines_order:
        all_machines_sorted = sorted(machines_order, key=machine_sort_key)
    else:
        all_machines_sorted = sorted(
            machines_df["Machine_ID"].unique(), key=machine_sort_key
        )

    df_real = schedule_df[schedule_df["Job_ID"] != "Idle"].copy() if "Job_ID" in schedule_df.columns else schedule_df.copy()
    if df_real.empty:
        return go.Figure().update_layout(
            title="No job data available",
            xaxis=dict(title="Time (minutes)"),
            yaxis=dict(title="Machine"),
        )

    x_min = df_real["Start_Time"].min()
    x_max = df_real["End_Time"].max()
    df_real["Start_Shifted"] = df_real["Start_Time"] - x_min
    df_real["End_Shifted"] = df_real["End_Time"] - x_min

    fig = go.Figure()

    colors = [
        "#636EFA", "#EF553B", "#00CC96", "#AB63FA", "#FFA15A",
        "#19D3F3", "#FF6692", "#B6E880", "#FF97FF", "#FECB52"
    ]
    color_map = {m: colors[i % len(colors)] for i, m in enumerate(all_machines_sorted)}

    for _, row in df_real.iterrows():
        machine = row["Machine_ID"]
        if machine not in all_machines_sorted:
            continue
        fig.add_trace(
            go.Bar(
                x=[row["End_Shifted"] - row["Start_Shifted"]],
                y=[machine],
                base=row["Start_Shifted"],
                orientation="h",
                marker_color=color_map[machine],
                hovertemplate=(
                    f"<b>Machine:</b> {machine}<br>"
                    f"Job ID: {row.get('Job_ID', 'N/A')}<br>"
                    f"Operation ID: {row.get('Operation_ID', 'N/A')}<br>"
                    f"Setup: {row.get('Setup_Time', 0)} min<br>"
                    f"Proc: {row.get('Proc_Time', 0)} min<br>"
                    f"Start: {row.get('Start_Time', 0):.0f} min<br>"
                    f"End: {row.get('End_Time', 0):.0f} min<extra></extra>"
                ),
                width=0.4,
                name=machine,
                showlegend=False,
            )
        )

    # Draw maintenance/breakdown windows (supports multiple windows per machine)
    for _, machine in machines_df.iterrows():
        maint = machine.get("Maintenance_Window")
        machine_id = machine.get("Machine_ID")
        
        if maint and machine_id in all_machines_sorted:
            # Handle both single window (dict) and multiple windows (list)
            windows = []
            if isinstance(maint, dict) and maint:
                windows = [maint]
            elif isinstance(maint, list):
                windows = [w for w in maint if isinstance(w, dict) and w]
            
            # Draw each maintenance/breakdown window
            for window in windows:
                if "start" in window and "end" in window:
                    fig.add_shape(
                        type="rect",
                        x0=window["start"] - x_min,
                        x1=window["end"] - x_min,
                        y0=all_machines_sorted.index(machine_id) - 0.4,
                        y1=all_machines_sorted.index(machine_id) + 0.4,
                        fillcolor="rgba(255,0,0,0.25)",
                        line=dict(color="red", width=2, dash="dash"),
                        layer="below",
                    )
                    
                    # Add label for breakdown window
                    fig.add_annotation(
                        x=(window["start"] + window["end"]) / 2 - x_min,
                        y=all_machines_sorted.index(machine_id),
                        text="üîß DOWN",
                        showarrow=False,
                        font=dict(size=9, color="red", family="Arial Black"),
                        bgcolor="rgba(255,255,255,0.7)",
                    )

    pad = max((x_max - x_min) * 0.05, 100)
    fig.update_layout(
        title=dict(text=title, font=dict(size=16, color='black')),
        xaxis=dict(
            title=dict(text="Time (minutes, shifted)", font=dict(size=14, color='black')),
            tickfont=dict(size=12, color='black'),
            range=[0 - pad, (x_max - x_min) + pad],
            showgrid=True
        ),
        yaxis=dict(
            title=dict(text="Machine", font=dict(size=14, color='black')),
            tickfont=dict(size=12, color='black'),
            categoryorder="array",
            categoryarray=all_machines_sorted,
            autorange="reversed",
            type="category"
        ),
        height=600,
        plot_bgcolor="white",
        paper_bgcolor="white",
        margin=dict(l=100, r=50, t=80, b=60),
        font=dict(size=12, color='black'),
    )
    return fig


def create_kpi_dashboard(_schedule_df, _df_ops_current, _machines_df, _heuristic_name="", _cache_key=None):
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots

    if _schedule_df is None or _schedule_df.empty:
        st.warning(f"No schedule data available to compute KPIs for {_heuristic_name}.")
        return go.Figure(), {
            'Makespan_Days': 0,
            'Total_Tardiness_Days': 0,
            'Total_Cost_$': 0,
            'On_Time_%': 0,
            'Machine_Utilization_%': 0
        }

    metrics = calculate_metrics(_schedule_df, _df_ops_current, _heuristic_name)

    makespan_days = metrics['Makespan_Days']
    tardiness_days = metrics['Total_Tardiness_Days']
    total_cost = metrics['Total_Cost_$']
    outsource_pct = (
        100 * _df_ops_current[_df_ops_current['Assignment_Type'] == 'OUTSOURCE']['Operation_ID'].nunique()
        / _df_ops_current['Operation_ID'].nunique()
        if len(_df_ops_current) > 0 else 0
    )
    ontime_pct = metrics['On_Time_%']
    utilization = metrics['Machine_Utilization_%']

    fig = make_subplots(
        rows=2, cols=3,
        subplot_titles=(
            'Makespan (Days)',
            'Total Tardiness (Days)',
            'Total Cost ($)',
            'Outsourced %',
            'On-Time Delivery % (Operations)',
            'Machine Utilization %'
        ),
        specs=[
            [{'type': 'indicator'}, {'type': 'indicator'}, {'type': 'indicator'}],
            [{'type': 'indicator'}, {'type': 'indicator'}, {'type': 'indicator'}]
        ]
    )

    fig.add_trace(go.Indicator(mode="number", value=round(makespan_days, 2)), row=1, col=1)
    fig.add_trace(go.Indicator(mode="number", value=round(tardiness_days, 2)), row=1, col=2)
    fig.add_trace(go.Indicator(mode="number", value=round(total_cost, 2), number={'prefix': "$"}), row=1, col=3)

    fig.add_trace(
        go.Indicator(mode="gauge+number", value=round(outsource_pct, 1), gauge={'axis': {'range': [0, 100]}, 'bar': {'color': "purple"}}),
        row=2, col=1
    )
    fig.add_trace(
        go.Indicator(mode="gauge+number", value=round(ontime_pct, 1), gauge={'axis': {'range': [0, 100]}, 'bar': {'color': "green"}}),
        row=2, col=2
    )
    fig.add_trace(
        go.Indicator(mode="gauge+number", value=round(utilization, 1), gauge={'axis': {'range': [0, 100]}, 'bar': {'color': "cyan"}}),
        row=2, col=3
    )

    fig.update_layout(height=400, margin=dict(t=50, b=10, l=10, r=10))
    return fig, metrics

# ---------------------------
# Operation status table
# ---------------------------

# ---------------------------
# Operation status table (UPDATED)
# ---------------------------

def create_operation_status_table(schedule_df, df_ops, _cache_key=None):
    """
    Create operation-level status table with safety checks.
    Handles cases where no schedule exists yet (before heuristic applied).
    """
    import pandas as pd, time

    # üß© SAFETY CHECK 1 ‚Äî schedule not available yet
    if schedule_df is None or schedule_df.empty:
        st.warning("‚ö†Ô∏è No schedule available yet. Please compute and apply a heuristic first to view operation status.")
        placeholder = pd.DataFrame({
            "Message": ["No scheduled operations found."],
            "Suggestion": ["Click 'Compute All Heuristics' in sidebar, then 'Apply Selected Heuristic'."]
        })
        return placeholder

    # üß© SAFETY CHECK 2 ‚Äî missing required column
    if 'Operation_ID' not in schedule_df.columns:
        st.warning("‚ö†Ô∏è Schedule is missing 'Operation_ID'. Please ensure a valid schedule was applied.")
        placeholder = pd.DataFrame({
            "Message": ["Invalid schedule data."],
            "Suggestion": ["Try reapplying a heuristic or recomputing all heuristics."]
        })
        return placeholder

    schedule_df = schedule_df.copy()
    df_ops = df_ops.copy()

    # Derive due times if not already present
    if 'Due_Time_Min' not in df_ops.columns:
        df_ops['Due_Time_Min'] = df_ops['Due_Day'] * 480

    op_status = []
    current_time = schedule_df['End_Time'].max() if not schedule_df.empty else 0

    for op_id in df_ops['Operation_ID'].unique():
        op_row_def = df_ops[df_ops['Operation_ID'] == op_id].iloc[0]
        op_row_sched = schedule_df[schedule_df['Operation_ID'] == op_id] \
            if 'Operation_ID' in schedule_df.columns else pd.DataFrame()

        due_time_min = op_row_def.get('Due_Time_Min', 0)
        total_proc_min = op_row_def.get('Total_Proc_Min', op_row_def.get('Proc_Time_per_Unit', 0) * op_row_def.get('Quantity', 1))
        assignment = op_row_def.get('Assignment_Type', 'IN_HOUSE')
        priority = int(op_row_def.get('Priority', 3))
        job_id = op_row_def.get('Job_ID', None)

        # <<< NEW: Variable to hold machine assignment >>>
        machine_assigned = 'N/A'

        # CASE 1: Scheduled
        if not op_row_sched.empty:
            finish_time = op_row_sched['End_Time'].max()
            finish_day = finish_time / 480
            tardiness_days = max(0, (finish_time - due_time_min) / 480)
            status = "On-Time" if tardiness_days == 0 else "Late"

            # <<< NEW: Get the assigned machine from the schedule >>>
            try:
                machine_assigned = op_row_sched['Machine_ID'].iloc[0]
            except IndexError:
                machine_assigned = 'Error'

        # CASE 2: Outsourced
        elif assignment == "OUTSOURCE":
            est_finish_time = op_row_def.get('Release_Time_Min', 0) + op_row_def.get('Outsource_Time_Min', 0)
            finish_time = est_finish_time
            finish_day = finish_time / 480
            tardiness_days = max(0, (finish_time - due_time_min) / 480)
            status = "Outsourced" if tardiness_days == 0 else "Outsource Delay"
            
            # <<< NEW: Set machine to 'OUTSOURCE' >>>
            machine_assigned = 'OUTSOURCE'

        # CASE 3: Pending
        else:
            finish_time = current_time
            finish_day = finish_time / 480

            if current_time > due_time_min:
                tardiness_days = round((current_time - due_time_min) / 480, 2)
                status = "Overdue (Pending)"
            else:
                tardiness_days = 0
                status = "Pending"

            # <<< NEW: Set machine to 'PENDING' >>>
            machine_assigned = 'PENDING'

        # Critical Ratio
        time_remaining = max(0, due_time_min - finish_time)
        cr_value = round(time_remaining / max(total_proc_min, 1), 2)

        op_status.append({
            'Job_ID': job_id,
            'Operation_ID': op_id,
            'Machine_ID': machine_assigned,  # <<< NEW: Added to dictionary >>>
            'Priority': priority,
            'Assignment': assignment,
            'Total_Proc_Min': round(total_proc_min, 2),
            'CR_Value': cr_value,
            'Finish_Day': round(finish_day, 2),
            'Due_Day': round(due_time_min / 480, 2),
            'Tardiness_Days': round(tardiness_days, 2),
            'Status': status,
            'Updated': time.strftime("%H:%M:%S")
        })

    op_status_df = pd.DataFrame(op_status)

    # <<< NEW: Re-order columns so Machine_ID is easy to see >>>
    column_order = [
        'Job_ID', 'Operation_ID', 'Machine_ID', 'Priority', 'Assignment',
        'Status', 'Total_Proc_Min', 'CR_Value', 'Finish_Day', 'Due_Day',
        'Tardiness_Days', 'Updated'
    ]
    # Keep only columns that actually exist
    final_columns = [col for col in column_order if col in op_status_df.columns]
    op_status_df = op_status_df[final_columns]


    # Sorting based on active heuristic
    heuristic = st.session_state.current_heuristic if 'current_heuristic' in st.session_state and st.session_state.current_heuristic else 'SPT'
    if heuristic == 'SPT':
        op_status_df = op_status_df.sort_values(['Priority', 'Total_Proc_Min']).reset_index(drop=True)
        sort_label = "Priority ‚Üí SPT (Total Processing Time)"
    elif heuristic == 'EDD':
        op_status_df = op_status_df.sort_values(['Priority', 'Due_Day']).reset_index(drop=True)
        sort_label = "Priority ‚Üí EDD (Due Day)"
    elif heuristic == 'CR':
        op_status_df = op_status_df.sort_values(['Priority', 'CR_Value']).reset_index(drop=True)
        sort_label = "Priority ‚Üí CR (Critical Ratio)"
    elif heuristic == 'PRIORITY':
        op_status_df = op_status_df.sort_values(['Priority']).reset_index(drop=True)
        sort_label = "Priority Only"
    else:
        op_status_df = op_status_df.sort_values(['Priority', 'Due_Day']).reset_index(drop=True)
        sort_label = "Priority ‚Üí Due Date (Fallback)"

    st.caption(f"üìã Active Sorting Rule: {sort_label}")
    return op_status_df

# ---------------------------
# EXPORT
# ---------------------------
def export_schedule(schedule_df):
    if schedule_df is None or schedule_df.empty:
        st.warning("‚ö†Ô∏è No schedule data available to export. Please compute and apply a heuristic first.")
        return b""

    export_df = schedule_df.copy()

    # Safety: ensure essential columns exist
    required_cols = ["Start_Time", "End_Time", "Tardiness"]
    for col in required_cols:
        if col not in export_df.columns:
            st.error(f"‚ùå Cannot export: missing column '{col}' in schedule.")
            return b""

    export_df['Start_Day'] = export_df['Start_Time'] / 480
    export_df['End_Day'] = export_df['End_Time'] / 480
    export_df['Tardiness_Days'] = export_df['Tardiness'] / 480 if 'Tardiness' in export_df.columns else 0

    export_df = export_df[[
        'Job_ID', 'Operation_ID', 'Machine_ID',
        'Start_Time', 'End_Time', 'Start_Day', 'End_Day',
        'Setup_Time', 'Proc_Time', 'Transfer_Time',
        'Due_Time', 'Tardiness', 'Tardiness_Days'
    ]]

    return export_df.to_csv(index=False).encode('utf-8')


# ---------------------------
# NEW: compute_all_heuristics_and_metrics + apply_heuristic_to_dataset
# ---------------------------
def compute_all_heuristics_and_metrics(ss, show_progress=True):
    heuristics = ['SPT', 'EDD', 'CR', 'PRIORITY']
    metrics = []
    schedules = {}

    if show_progress:
        st.info("üîÑ Computing schedules for all heuristics using the CURRENT dataset "
                "(including any breakdowns, priority, or outsourcing updates)...")
        progress = st.progress(0)

    # ‚úÖ Always use latest state
    ss.base_df_ops = ss.df_ops.copy()
    ss.base_df_machines = ss.df_machines.copy()

    # ‚úÖ Clear resource cache to force re-run
    try:
        st.cache_resource.clear()
    except Exception:
        pass

    # ‚úÖ Compute all heuristics freshly
    for i, heur in enumerate(heuristics):
        schedule_key = f"schedule_{heur.lower()}"
        try:
            schedule = run_single_heuristic(
                ss.base_df_ops.copy(),
                ss.base_df_machines.copy(),
                ss.base_df_effective.copy(),
                ss.base_df_penalties.copy(),
                heuristic=heur,
            )
            schedules[heur] = schedule.copy()
            setattr(ss, schedule_key, schedule.copy())
            metrics.append(calculate_metrics(schedule.copy(), ss.base_df_ops.copy(), heur))
            
            # Log successful computation
            log_action(ss, "HEURISTIC_COMPUTED", {
                'heuristic': heur,
                'ops_scheduled': len(schedule)
            })
        except Exception as e:
            dbg(f"‚ö†Ô∏è compute_all failed for {heur}: {e}")
            existing = getattr(ss, schedule_key, pd.DataFrame())
            metrics.append(calculate_metrics(existing, ss.base_df_ops.copy(), heur))

        if show_progress:
            progress.progress((i + 1) / len(heuristics))

    # ‚úÖ Store updated metrics
    ss.df_metrics = pd.DataFrame(metrics)
    ss.schedule_update_key = f"compute_all_{int(time.time())}"
    ss.recalculate_all_heuristics = False
    ss.force_metric_refresh = False

    # ‚úÖ Notify success
    st.success("‚úÖ All heuristics recomputed successfully.")
    safe_toast("üìä Updated heuristic metrics ready for review!", icon="üìà")

    # ‚úÖ Clear any pending prompts (from breakdown / priority / outsourcing)
    ss.breakdown_pending = False
    ss.breakdown_message_visible = False
    if hasattr(ss, "priority_update_message_visible"):
        ss.priority_update_message_visible = False
    if hasattr(ss, "outsourcing_update_message_visible"):
        ss.outsourcing_update_message_visible = False

    # ‚úÖ Always redirect to comparison page after recomputation
    ss.current_page = "comparison"

    # ‚úÖ Preserve user‚Äôs current heuristic (no default to SPT)
    current_h = ss.get("current_heuristic", None)
    if not current_h or current_h not in heuristics:
        st.warning("‚ö†Ô∏è No heuristic selected. Please select one from the comparison table.")
        ss.current_heuristic = None
        ss.last_applied_heuristic = None
        ss.current_schedule = pd.DataFrame()
        return schedules, ss.df_metrics

    # ‚úÖ Save back for persistence
    ss.current_heuristic = current_h
    ss.last_applied_heuristic = current_h

    # ‚úÖ Sync current heuristic‚Äôs schedule
    schedule_key = f"schedule_{current_h.lower()}"
    if hasattr(ss, schedule_key):
        ss.current_schedule = getattr(ss, schedule_key).copy()
        safe_toast(f"üîÅ Loaded updated schedule for {current_h}", icon="üìà")
    else:
        st.warning(f"‚ö†Ô∏è No schedule found for {current_h}. Please select a heuristic.")
        ss.current_schedule = pd.DataFrame()

    # ‚úÖ Guarantee essential columns exist
    required_cols = ['Job_ID', 'Machine_ID', 'Start_Time', 'End_Time']
    for col in required_cols:
        if col not in ss.current_schedule.columns:
            ss.current_schedule[col] = []

    # ‚úÖ Force KPI refresh
    ss.schedule_update_key = str(time.time())
    st.session_state.schedule_update_key = ss.schedule_update_key

    # ‚úÖ Re-render app with latest state
    st.rerun()

    return schedules, ss.df_metrics



def apply_heuristic_to_dataset(ss, heuristic):
    schedule_key = f"schedule_{heuristic.lower()}"
    schedule_df = getattr(ss, schedule_key, None)

    if schedule_df is None or schedule_df.empty:
        st.error(f"No computed schedule found for {heuristic}. Click 'Compute Heuristics' first.")
        return False

    sched = schedule_df.copy()
    if 'Operation_ID' not in sched.columns:
        st.error("Schedule missing Operation_ID ‚Äî cannot apply.")
        return False

    sched = sched[['Operation_ID', 'Job_ID', 'Machine_ID', 'Start_Time', 'End_Time', 'Tardiness']].copy()
    ops = ss.base_df_ops.copy()

    ops = ops.merge(sched, on='Operation_ID', how='left', suffixes=('', '_sched'))

    for col in ['Machine_ID', 'Start_Time', 'End_Time', 'Tardiness']:
        if f"{col}_sched" in ops.columns:
            ops[col] = ops[f"{col}_sched"].combine_first(ops.get(col))

    ops = ops.drop(columns=[c for c in ops.columns if c.endswith('_sched')], errors='ignore')

    ops['Completion_Day'] = ops.apply(
        lambda r: (r['End_Time'] / 480) if pd.notna(r.get('End_Time')) else r.get('Completion_Day', 0),
        axis=1
    )
    ops['Assigned_By_Heuristic'] = heuristic

    ss.base_df_ops = ops.copy()
    ss.df_ops = ops.copy()

    # ‚úÖ Persist current heuristic choice
    ss.current_heuristic = heuristic
    ss.last_applied_heuristic = heuristic
    ss.current_schedule = schedule_df.copy()

    # üß© Immediately sync this heuristic to current view
    ss.df_ops = ss.base_df_ops.copy()
    ss.df_machines = ss.base_df_machines.copy()
    ss.schedule_update_key = str(time.time())

    ss.force_metric_refresh = False
    ss.recalculate_all_heuristics = False

    # üß© Recompute KPIs for the applied heuristic
    try:
        st.info(f"üìä Recomputing KPI metrics for {heuristic} ...")
        metrics_df = calculate_metrics(
            ss.current_schedule.copy(),
            ss.df_ops.copy(),
            heuristic
        )

        # ‚úÖ Update or create metrics table
        if hasattr(ss, "df_metrics") and ss.df_metrics is not None:
            ss.df_metrics = pd.concat([
                ss.df_metrics[ss.df_metrics["Heuristic"] != heuristic],
                pd.DataFrame([metrics_df])
            ], ignore_index=True)
        else:
            ss.df_metrics = pd.DataFrame([metrics_df])

        st.success(f"üìà KPI metrics updated for {heuristic}.")
        
        # Log the action
        log_action(ss, "HEURISTIC_APPLIED", {
            'heuristic': heuristic,
            'ops_applied': len(ss.current_schedule)
        })
    except Exception as e:
        st.error(f"‚ùå Failed to update KPI metrics for {heuristic}: {e}")

    # ‚úÖ Sync selected heuristic‚Äôs schedule across all
    for h in ['SPT', 'EDD', 'CR', 'PRIORITY']:
        setattr(ss, f'schedule_{h.lower()}', ss.current_schedule.copy())

    st.success(f"‚úÖ {heuristic} schedule copied to all heuristics for next operation updates.")

    # ‚úÖ Switch to detailed heuristic view
    ss.current_page = "heuristic_view"
    ss.force_metric_refresh = True
    ss.schedule_update_key = str(time.time())

    st.rerun()

# ---------------------------
# Sidebar widgets & controls
# ---------------------------
def draw_compute_apply_controls(ss):
    st.sidebar.markdown("### üßÆ Compute & Apply Heuristics")
    if st.sidebar.button("üß™ Compute All Heuristics (current dataset)", key="compute_all_heurs"):
        compute_all_heuristics_and_metrics(ss, show_progress=True)
        st.rerun()

    heuristic_options = ('SPT', 'EDD', 'CR', 'PRIORITY')
    
    # --- START FIX ---
    # This logic IDENTICALLY matches the logic in your draw_heuristic_selector
    # It finds the correct index if a heuristic is selected
    # or defaults the *display* to index 0 if ss.current_heuristic is None.
    # It does NOT change ss.current_heuristic.
    
    current_h = ss.get("current_heuristic") # Get value (could be None)
    
    if current_h in heuristic_options:
        current_index = heuristic_options.index(current_h)
    else:
        current_index = 0 # Default display to 'SPT' if state is None
    # --- END FIX ---

    apply_choice = st.sidebar.selectbox(
        "Choose heuristic to APPLY (persist):", 
        heuristic_options,
        index=current_index, # Use the synced index
        key="apply_choice"
    )
    
    if st.sidebar.button("‚úÖ Apply Selected Heuristic", key="apply_heur"):
        ok = apply_heuristic_to_dataset(ss, apply_choice)
        if ok:
            st.rerun()


# ---------------------------
# Existing sidebar widgets (modified to persist base data changes)
# ---------------------------
def draw_heuristic_selector(ss):
    st.sidebar.markdown("### üéØ Scheduling Algorithm")
    st.sidebar.info("**EXPLAINER**: Different algorithms prioritize different factors:\n"
                    "- **SPT**: Shortest jobs first (fast completion)\n"
                    "- **EDD**: Due dates first (minimize lateness)\n"
                    "- **CR**: Critical ratio (balance time & work)\n"
                    "- **PRIORITY**: High priority jobs first")

    heuristic_options = ('SPT', 'EDD', 'CR', 'PRIORITY')
    
    # 1. Ensure current_heuristic is valid, default to SPT if None
    if ss.current_heuristic not in heuristic_options:
        ss.current_heuristic = 'SPT'

    # 2. CRITICAL FIX: Sync the widget's internal state to match ss.current_heuristic
    # This prevents the widget from reverting to 'SPT' (or a previous value) 
    # when interactions happen elsewhere (like the Breakdown dropdown).
    if 'heuristic_selector' not in st.session_state:
        st.session_state.heuristic_selector = ss.current_heuristic
    elif st.session_state.heuristic_selector != ss.current_heuristic:
        st.session_state.heuristic_selector = ss.current_heuristic

    # 3. Draw the selectbox
    # Note: We don't need 'index' here because setting st.session_state['heuristic_selector'] 
    # sets the selected value automatically.
    selected_heuristic = st.sidebar.selectbox(
        "Select Scheduling Algorithm (view only)",
        heuristic_options,
        key='heuristic_selector'
    )

    # 4. Handle User Interaction
    # Only update if the returned value differs from the state 
    # (The sync in step 2 ensures this only happens if the USER actually changed this specific dropdown)
    if selected_heuristic != ss.current_heuristic:
        st.sidebar.write(f"üîÑ Switching view to {selected_heuristic} (no apply).")
        schedule_key = f'schedule_{selected_heuristic.lower()}'
        
        if getattr(ss, schedule_key, None) is None:
            st.sidebar.warning(f"No computed schedule for {selected_heuristic}. Use 'Compute All Heuristics' first.")
            # Revert the dropdown visual to the valid heuristic to avoid confusion
            st.session_state.heuristic_selector = ss.current_heuristic
        else:
            ss.current_heuristic = selected_heuristic
            ss.current_schedule = getattr(ss, schedule_key).copy()
            # set working copies to base data (dataset remains canonical until user applies)
            ss.df_ops = ss.base_df_ops.copy()
            ss.df_machines = ss.base_df_machines.copy()
            safe_toast(f"üìä Viewing {ss.current_heuristic} schedule (not applied).", icon="‚ÑπÔ∏è")
            st.rerun()

def draw_live_job_scheduler(ss):
    st.write("**‚ûï Add New Job**")
    
    col1, col2 = st.columns(2)
    with col1:
        live_job_id = st.text_input("Job ID:", f"J{900 + len(ss.df_ops['Job_ID'].unique())}", key='live_job_id')
        live_quantity = st.number_input("Quantity:", min_value=10, max_value=1000, value=100, step=10, key='live_qty')
    with col2:
        live_priority = st.selectbox("Priority:", [1, 2, 3], index=0, key='live_priority')
        live_due_days = st.number_input("Due (days):", min_value=1, max_value=30, value=7, step=1, key='live_due')

    live_op_count = st.slider("Operations:", 1, 5, 2, key='live_op_count')

    operations_config = []
    for i in range(live_op_count):
        with st.container():
            col_a, col_b, col_c, col_d = st.columns(4)
            with col_a:
                op_type = st.selectbox(f"Op{i+1}:", ['MILLING', 'TURNING', 'GRINDING', 'DRILLING'], key=f'live_op{i}_type')
            with col_b:
                material = st.selectbox(f"Mat:", ['STEEL', 'ALUM', 'TITAN', 'BRASS'], key=f'live_op{i}_mat')
            with col_c:
                proc_time = st.number_input(f"Time:", min_value=0.1, max_value=5.0, value=0.3, step=0.1, key=f'live_op{i}_time')
            with col_d:
                setup_time = st.number_input(f"Setup:", min_value=10, max_value=120, value=30, step=5, key=f'live_op{i}_setup')

            operations_config.append({
                'op_type': op_type,
                'material': material,
                'proc_time': proc_time,
                'setup_time': setup_time
            })

    col_analyze, col_add = st.columns(2)
    with col_analyze:
        if st.button("üîç Analyze", key='analyze_capacity_button', use_container_width=True):
            current_time_days = ss.current_schedule['End_Time'].max() / 480 if not ss.current_schedule.empty else 0
            release_time_min = current_time_days * 480
            due_time_min = release_time_min + (live_due_days * 480)

            new_job_ops = []
            for i, op_config in enumerate(operations_config):
                new_op_id = f'{live_job_id}_Op{i+1}'
                new_op = {
                    'Job_ID': live_job_id,
                    'Operation_ID': new_op_id,
                    'Op_Seq': i + 1,
                    'Part_Type': f'NEW_{live_job_id}',
                    'Quantity': live_quantity,
                    'Op_Type': op_config['op_type'],
                    'Mat_Type': op_config['material'],
                    'Tool_Group': 'TGA',
                    'Proc_Time_per_Unit': op_config['proc_time'],
                    'Setup_Time': op_config['setup_time'],
                    'Transfer_Min': 5,
                    'Release_Day': current_time_days,
                    'Due_Day': current_time_days + live_due_days,
                    'Priority': live_priority,
                    'Outsource_Flag': 'Y',
                    'Vendor_Ref': 'V1' if 'V1' in ss.base_df_vendors['Vendor_ID'].values else None,
                    'Release_Time_Min': release_time_min,
                    'Due_Time_Min': due_time_min,
                    'Total_Proc_Min': op_config['proc_time'] * live_quantity
                }
                new_job_ops.append(new_op)

            new_eff_times = []
            for op in new_job_ops:
                eligible_machines = get_eligible_machines(op['Op_Type'])
                if not eligible_machines:
                    continue

                for machine_id in eligible_machines:
                    machine = ss.base_df_machines[ss.base_df_machines['Machine_ID'] == machine_id].iloc[0]
                    speed_factor = float(machine['Speed Factor'])
                    oee = float(machine['OEE (Uptime)'])
                    effective_time = op['Total_Proc_Min'] * speed_factor * (1 / oee)
                    total_time = effective_time + op['Setup_Time'] + op['Transfer_Min']

                    new_eff_times.append({
                        'Operation_ID': op['Operation_ID'],
                        'Machine_ID': machine_id,
                        'Effective_Proc_Time': effective_time,
                        'Setup_Time': op['Setup_Time'],
                        'Transfer_Min': op['Transfer_Min'],
                        'Total_Time': total_time
                    })

            df_new_effective = pd.DataFrame(new_eff_times)

            analysis = analyze_capacity_for_new_job(
                new_job_ops,
                ss.current_schedule,
                ss.df_machines,
                df_new_effective,
                due_time_min
            )

            ss.live_job_analysis = analysis
            ss.live_job_ops_pending = new_job_ops
            ss.live_job_effective_pending = df_new_effective
            st.rerun()

        # Analysis results display
        if hasattr(ss, 'live_job_analysis') and ss.live_job_analysis:
            st.divider()
            analysis = ss.live_job_analysis
            
            if analysis['recommendation'] == 'SCHEDULE':
                st.success("‚úÖ Can schedule in-house")
            else:
                st.error("‚ùå Recommend outsourcing")
            
            st.caption(f"Completion: Day {analysis['metrics'].get('new_job_time_days', 0):.1f} ‚Ä¢ "
                      f"Utilization: {analysis['metrics'].get('projected_utilization', 0):.1f}%")

        with col_add:
            if st.button("‚ûï Add Job", key='add_job_button', use_container_width=True, disabled=not hasattr(ss, 'live_job_analysis')):
                # Validation check
                job_id_to_add = ss.get('live_job_id', '') 
                if job_id_to_add in ss.base_df_ops['Job_ID'].values:
                    st.error(f"Job '{job_id_to_add}' exists!")
                    st.stop()

                with st.spinner(f"Adding job..."):
                    current_time_days_add = ss.current_schedule['End_Time'].max() / 480 if not ss.current_schedule.empty else 0
                    live_due_days_add = ss.get('live_due', 7) 
                    new_release_time_min = current_time_days_add * 480
                    new_due_time_min = new_release_time_min + (live_due_days_add * 480)

                    df_new_ops = pd.DataFrame(ss.live_job_ops_pending)
                    df_new_ops['Job_ID'] = job_id_to_add 
                    df_new_ops['Release_Day'] = current_time_days_add
                    df_new_ops['Due_Day'] = current_time_days_add + live_due_days_add
                    df_new_ops['Release_Time_Min'] = new_release_time_min
                    df_new_ops['Due_Time_Min'] = new_due_time_min
                    
                    if analysis['recommendation'] == 'OUTSOURCE':
                        df_new_ops['Assignment_Type'] = 'OUTSOURCE'
                        df_new_ops['Outsource_Cost'] = df_new_ops['Quantity'] * 5.0
                        df_new_ops['Outsource_Time_Min'] = live_due_days_add * 480 * 0.8
                    else:
                        df_new_ops['Assignment_Type'] = 'IN_HOUSE'
                        df_new_ops['Outsource_Cost'] = 0
                        df_new_ops['Outsource_Time_Min'] = 0

                    ss.df_ops = pd.concat([ss.df_ops, df_new_ops], ignore_index=True)
                    ss.base_df_ops = pd.concat([ss.base_df_ops, df_new_ops], ignore_index=True)
                    ss.base_df_effective = pd.concat([ss.base_df_effective, ss.live_job_effective_pending], ignore_index=True)

                    # Log the action
                    log_action(ss, "JOB_ADDED", {
                        'job_id': job_id_to_add,
                        'op_count': len(df_new_ops),
                        'priority': df_new_ops.iloc[0].get('Priority', 'N/A'),
                        'due_days': live_due_days_add,
                        'assignment': analysis['recommendation']
                    })

                    scheduler_new = CNCScheduler(ss.df_ops, ss.df_machines, ss.base_df_effective, ss.base_df_penalties)
                    ss.current_schedule = scheduler_new.run_scheduling(heuristic=ss.current_heuristic)

                    del ss.live_job_analysis
                    del ss.live_job_ops_pending
                    del ss.live_job_effective_pending

                    st.cache_data.clear()
                    safe_toast(f"‚úÖ Job {job_id_to_add} added!", icon="üéØ")
                st.rerun()

def draw_job_deleter(ss):
    st.write("**üóëÔ∏è Delete Job**")
    all_jobs = sorted(ss.df_ops['Job_ID'].unique())
    
    if len(all_jobs) == 0:
        st.write("No jobs to delete.")
        return
    
    job_to_delete = st.selectbox("Select job:", all_jobs, key='delete_job_select')
    
    if st.button("Delete", key='delete_job_button', type="secondary", use_container_width=True):
        with st.spinner(f"Deleting {job_to_delete}..."):
            # Find all operations to delete
            ops_to_delete = ss.df_ops[ss.df_ops['Job_ID'] == job_to_delete]['Operation_ID'].unique()
            op_count = len(ops_to_delete)
            
            # Remove from dataframes
            ss.df_ops = ss.df_ops[ss.df_ops['Job_ID'] != job_to_delete].copy()
            ss.base_df_ops = ss.base_df_ops[ss.base_df_ops['Job_ID'] != job_to_delete].copy()
            ss.base_df_effective = ss.base_df_effective[~ss.base_df_effective['Operation_ID'].isin(ops_to_delete)].copy()
            
            # Log the action
            log_action(ss, "JOB_DELETED", {
                'job_id': job_to_delete,
                'op_count': op_count
            })
            
            # Recompute schedule if heuristic is active
            if ss.current_heuristic:
                scheduler = CNCScheduler(ss.df_ops, ss.df_machines, ss.base_df_effective, ss.base_df_penalties)
                ss.current_schedule = scheduler.run_scheduling(heuristic=ss.current_heuristic)
            else:
                ss.current_schedule = pd.DataFrame()
                ss.recalculate_all_heuristics = True
            
            st.cache_data.clear()
            safe_toast(f"‚úÖ Deleted {job_to_delete}", icon="üóëÔ∏è")
        st.rerun()

def draw_breakdown_simulator(ss):
    st.write("**‚öôÔ∏è Machine Breakdown Simulator**")
    
    # Get list of machines
    available_machines = sorted(ss.df_machines['Machine_ID'].unique())
    
    col1, col2 = st.columns(2)
    with col1:
        breakdown_machine = st.selectbox("Machine:", available_machines, key='breakdown_machine')
    with col2:
        input_mode = st.radio("Input Mode:", ["Day & Hour", "Minutes"], key='breakdown_input_mode', horizontal=True)
    
    if input_mode == "Day & Hour":
        col3, col4 = st.columns(2)
        with col3:
            breakdown_day = st.number_input("Day:", min_value=1, max_value=50, value=10, step=1, key='breakdown_day')
            breakdown_start_hour = st.number_input("Start Hour (8-16):", min_value=8, max_value=16, value=10, step=1, key='breakdown_start')
        with col4:
            breakdown_duration = st.number_input("Duration (hours):", min_value=1, max_value=8, value=2, step=1, key='breakdown_duration')
        
        # Calculate minutes
        MINUTES_PER_WORKDAY = 8 * 60
        start_min_in_day = (breakdown_start_hour - 8) * 60
        start_time_min = (breakdown_day - 1) * MINUTES_PER_WORKDAY + start_min_in_day
        duration_min = breakdown_duration * 60
        
        st.caption(f"üìä Breakdown window: {start_time_min} to {start_time_min + duration_min} minutes")
    else:
        col3, col4 = st.columns(2)
        with col3:
            start_time_min = st.number_input("Start Time (minutes):", min_value=0, max_value=50000, value=10000, step=100, key='breakdown_start_min')
        with col4:
            duration_min = st.number_input("Duration (minutes):", min_value=60, max_value=480, value=120, step=60, key='breakdown_duration_min')
        
        # Calculate day/hour for display
        breakdown_day = (start_time_min // 480) + 1
        start_hour = 8 + ((start_time_min % 480) // 60)
        end_min = start_time_min + duration_min
        end_hour = 8 + ((end_min % 480) // 60)
        
        st.caption(f"üìä Equivalent to: Day {breakdown_day}, Hour {start_hour}:00-{end_hour}:00")
    
    if st.button("üîß Add Breakdown", key='add_breakdown_btn', use_container_width=True):
        # Calculate breakdown window in minutes
        end_time_min = start_time_min + duration_min
        
        # Create breakdown window
        breakdown_window = {
            'start': start_time_min,
            'end': end_time_min,
            'duration': duration_min
        }
        
        # Update machine dataframe
        machine_idx = ss.df_machines[ss.df_machines['Machine_ID'] == breakdown_machine].index[0]
        
        # Get existing maintenance windows
        existing_maintenance = ss.df_machines.loc[machine_idx, 'Maintenance_Window']
        
        # Append new breakdown to existing windows (convert to list)
        if existing_maintenance is None or (isinstance(existing_maintenance, dict) and not existing_maintenance):
            new_maintenance = [breakdown_window]
        elif isinstance(existing_maintenance, dict):
            new_maintenance = [existing_maintenance, breakdown_window]
        elif isinstance(existing_maintenance, list):
            new_maintenance = existing_maintenance + [breakdown_window]
        else:
            new_maintenance = [breakdown_window]
        
        # Update both working and base dataframes - use .at[] to avoid ValueError with lists
        ss.df_machines.at[machine_idx, 'Maintenance_Window'] = new_maintenance
        ss.base_df_machines.at[machine_idx, 'Maintenance_Window'] = new_maintenance
        
        # Set flag to recompute all heuristics
        ss.recalculate_all_heuristics = True
        ss.breakdown_pending = True
        
        # Log the action
        log_action(ss, "BREAKDOWN_ADDED", {
            'machine': breakdown_machine,
            'start_min': start_time_min,
            'end_min': end_time_min,
            'duration_min': duration_min,
            'day': (start_time_min // 480) + 1
        })
        
        # Calculate display values
        display_day = (start_time_min // 480) + 1
        display_start_hour = 8 + ((start_time_min % 480) // 60)
        display_end_hour = 8 + ((end_time_min % 480) // 60)
        
        st.success(f"‚úÖ Breakdown added to {breakdown_machine}:")
        st.info(f"   üìÖ Day {display_day}, {display_start_hour}:00-{display_end_hour}:00\n"
                f"   ‚è±Ô∏è Minutes: {start_time_min} to {end_time_min} ({duration_min} min)")
        st.info("üí° Click **'üß™ Compute All Heuristics'** to see updated schedules with breakdown impact")
        
        safe_toast(f"‚öôÔ∏è Breakdown registered for {breakdown_machine}", icon="üîß")
    
    # Show current breakdowns/maintenance
    if st.checkbox("Show Current Maintenance/Breakdowns", key='show_breakdowns'):
        st.caption("**Current Maintenance/Breakdown Windows:**")
        for _, machine in ss.df_machines.iterrows():
            machine_id = machine['Machine_ID']
            maintenance = machine.get('Maintenance_Window', None)
            
            if maintenance and maintenance is not None:
                if isinstance(maintenance, dict) and maintenance:
                    windows = [maintenance]
                elif isinstance(maintenance, list):
                    windows = maintenance
                else:
                    windows = []
                
                if windows:
                    st.write(f"**{machine_id}:**")
                    for i, window in enumerate(windows):
                        start_min = window.get('start', 0)
                        end_min = window.get('end', 0)
                        start_day = (start_min // 480) + 1
                        start_hour = 8 + ((start_min % 480) // 60)
                        end_hour = 8 + ((end_min % 480) // 60)
                        st.caption(f"  ‚Ä¢ Day {start_day}, {start_hour}:00-{end_hour}:00 ({(end_min - start_min) // 60}h)")
        
        if st.button("üóëÔ∏è Clear All Breakdowns", key='clear_breakdowns'):
            # Reset maintenance to original (from CSV)
            cleared_count = 0
            for idx, machine in ss.df_machines.iterrows():
                machine_id = machine['Machine_ID']
                # Parse original maintenance from CSV column
                original_maint_str = machine.get('Scheduled Maintenance (Day, Time-Time)', 'None')
                parsed_maint = parse_maintenance(original_maint_str)
                ss.df_machines.at[idx, 'Maintenance_Window'] = parsed_maint
                ss.base_df_machines.at[idx, 'Maintenance_Window'] = parsed_maint
                cleared_count += 1
            
            # Log the action
            log_action(ss, "BREAKDOWNS_CLEARED", {'machines_affected': cleared_count})
            
            ss.recalculate_all_heuristics = True
            st.success("‚úÖ All dynamic breakdowns cleared. Reverted to original maintenance schedule.")
            st.rerun()

def draw_priority_manager(ss):
    st.write("**üìä Job Priority**")
    all_jobs = sorted(ss.df_ops['Job_ID'].unique())
    if len(all_jobs) == 0:
        st.write("No jobs available")
        return
    
    col1, col2 = st.columns(2)
    with col1:
        job_sel = st.selectbox("Job:", all_jobs, key='priority_job')
    with col2:
        new_priority = st.radio("Priority:", [1, 2, 3, 4], horizontal=True, key='priority_val')
    
    if st.button("Update Priority", key='priority_btn', use_container_width=True):
        # Get old priority for logging
        old_priority = ss.df_ops[ss.df_ops['Job_ID'] == job_sel]['Priority'].iloc[0] if len(ss.df_ops[ss.df_ops['Job_ID'] == job_sel]) > 0 else 'N/A'
        
        ss.df_ops.loc[ss.df_ops['Job_ID'] == job_sel, 'Priority'] = new_priority
        ss.base_df_ops.loc[ss.base_df_ops['Job_ID'] == job_sel, 'Priority'] = new_priority
        
        # Log the action
        log_action(ss, "PRIORITY_CHANGED", {
            'job_id': job_sel,
            'old_priority': old_priority,
            'new_priority': new_priority
        })
        
        safe_toast(f"‚úÖ {job_sel} priority set to {new_priority}", icon="üìä")
        st.rerun()

def draw_outsourcing_policy(ss):
    st.write("**üè≠ Outsourcing Cost Threshold**")
    threshold = st.slider(
        "Cost threshold (vendor must be < threshold √ó in-house cost):",
        min_value=0.5,
        max_value=1.0,
        value=ss.get('cost_threshold', 0.9),
        step=0.05,
        help="Lower value = less outsourcing. E.g., 0.8 means outsource only if vendor is <80% of in-house cost",
        key='outsource_threshold_slider'
    )
    
    if st.button("Apply Threshold", key='apply_threshold'):
        ss.cost_threshold = threshold
        log_action(ss, "OUTSOURCING_UPDATED", {
            'threshold': threshold,
            'description': f"Cost threshold set to {threshold:.0%}"
        })
        st.success(f"‚úÖ Cost threshold updated to {threshold:.0%}. Recompute algorithms to apply changes.")
        st.rerun()
    
    st.caption(f"**Current:** {ss.get('cost_threshold', 0.9):.0%} ‚Äî Lower = More In-House")
    
    # Quick recommendations
    with st.expander("üìä Threshold Impact Guide"):
        st.markdown("""
        **Recommended Thresholds:**
        - **50-60%**: Maximum in-house utilization (only outsource if vendor is 50% cheaper)
        - **70-80%**: Balanced approach (recommended for reducing current 63% outsourcing)
        - **90-95%**: Cost-focused (current setting - high outsourcing)
        - **100%**: Outsource at cost parity
        
        **Current Issue**: At 90%, you're outsourcing whenever vendor is slightly cheaper, 
        ignoring the value of keeping work in-house (overhead absorption, quality control).
        
        **Recommendation**: Start with **75%** and monitor results.
        """)

# ---------------------------
# Main page content functions
# ---------------------------
def draw_kpi_dashboard(ss):
    if ss.get("current_heuristic"):
        st.header(f"üìä KPI Dashboard ({ss.current_heuristic})")
    else:
        st.warning("‚ö†Ô∏è No heuristic selected. Please compute and select one from the comparison page.")

    st.caption(f"üß† DEBUG: KPI source rows = {len(ss.current_schedule) if ss.current_schedule is not None else 0}")

    
    # üö® Force unique cache key each render to bypass stale cache
    _cache_key = str(time.time())

    kpi_fig, metrics = create_kpi_dashboard(
        ss.current_schedule if ss.current_schedule is not None else pd.DataFrame(),
        ss.df_ops,
        ss.df_machines,
        ss.current_heuristic or "",
        _cache_key=_cache_key
    )

    st.plotly_chart(kpi_fig, use_container_width=True)
    st.caption(f"Last Updated: {time.strftime('%H:%M:%S')} ({ss.current_heuristic})")

    col1, col2, col3 = st.columns(3)
    col1.metric("Makespan (Days)", metrics['Makespan_Days'])
    col2.metric("Total Tardiness (Days)", metrics['Total_Tardiness_Days'])
    col3.metric("On-Time %", metrics['On_Time_%'])
    
    # Show breakdown/maintenance impact
    total_breakdown_time = 0
    machines_with_breakdowns = []
    for _, machine in ss.df_machines.iterrows():
        maint = machine.get('Maintenance_Window')
        if maint:
            windows = []
            if isinstance(maint, dict) and maint:
                windows = [maint]
            elif isinstance(maint, list):
                windows = [w for w in maint if isinstance(w, dict) and w]
            
            if windows:
                machines_with_breakdowns.append(machine['Machine_ID'])
                for window in windows:
                    total_breakdown_time += window.get('duration', 0)
    
    if machines_with_breakdowns:
        st.info(f"üîß **Breakdown Impact**: {len(machines_with_breakdowns)} machine(s) have maintenance/breakdown windows "
                f"({', '.join(machines_with_breakdowns)}) ‚Äî Total downtime: {total_breakdown_time/60:.1f} hours")
    
    # --- AI-POWERED PERFORMANCE INSIGHTS ---
    with st.expander("ü§ñ AI Performance Analysis & Recommendations", expanded=False):
        if st.checkbox("Generate AI Insights", key='kpi_ai_insights'):
            with st.spinner("üß† Analyzing performance..."):
                context = f"""
                CURRENT SCHEDULE PERFORMANCE ({ss.current_heuristic}):
                - Makespan: {metrics['Makespan_Days']:.2f} days
                - Total Tardiness: {metrics['Total_Tardiness_Days']:.2f} days
                - On-Time Delivery: {metrics['On_Time_%']:.1f}%
                - Machine Utilization: {metrics['Machine_Utilization_%']:.1f}%
                - Total Cost: ${metrics['Total_Cost_$']:.2f}
                - Total Operations: {len(ss.df_ops)}
                - Machines: {len(ss.df_machines)}
                
                Analyze this CNC manufacturing schedule and provide:
                1. PERFORMANCE ASSESSMENT: Is this schedule optimal? (2-3 sentences)
                2. TOP 3 BOTTLENECKS: What's limiting performance?
                3. QUICK WINS: 2-3 actionable improvements
                4. RISK ALERT: Any concerning metrics that need immediate attention?
                
                Be specific, actionable, and focused on manufacturing operations.
                """
                
                insights = generate_ai_insights(
                    "Provide detailed performance analysis and recommendations.",
                    context_data=context
                )
                
                if insights:
                    st.markdown(insights)
                    st.caption("üí° Analysis powered by Google Gemini AI")
                else:
                    st.info("üí° AI insights require `google-generativeai` package and valid API key in .env file")


def draw_gantt_tab(ss):
    st.header(f"üìà Gantt Chart ({ss.current_heuristic or 'N/A'})")
    with st.spinner("Generating Gantt chart..."):
        _cache_key = str(time.time())

        gantt_fig = create_gantt_chart(
            ss.current_schedule if not ss.current_schedule is None else pd.DataFrame(),
            ss.df_machines,
            f"{ss.current_heuristic or 'N/A'} Schedule",
            _cache_key=_cache_key,
            machines_order=ss.machine_order
        )
        st.plotly_chart(gantt_fig, use_container_width=True)

def draw_operation_status_tab(ss):
    st.header(f"üìã Operation Status ({ss.current_heuristic or 'N/A'})")
    with st.spinner("Generating operation status table..."):
        _cache_key = getattr(ss, "schedule_update_key", str(time.time()))
        status_table = create_operation_status_table(ss.current_schedule.copy() if not ss.current_schedule is None else pd.DataFrame(), ss.df_ops.copy(), _cache_key=_cache_key)

    st.info(f"üîÑ Cache Key: {getattr(ss, 'schedule_update_key', 'N/A')}")
    st.dataframe(status_table, use_container_width=True, height=500)

def draw_comparison_tab(ss):
    
    # === AI & SETTINGS SECTION ===
    col1, col2 = st.columns([2, 1])
    
    with col1:
        with st.expander("ü§ñ AI-Powered Analysis", expanded=False):
            tab1, tab2 = st.tabs(["Dataset Quality", "Algorithm Insights"])
            
            with tab1:
                if st.checkbox("Analyze Dataset Quality", key='ai_data_quality'):
                    with st.spinner("üß† Analyzing..."):
                        df_ops = ss.base_df_ops
                        
                        # Calculate breakdown info
                        total_breakdown_hours = 0
                        machines_with_breakdowns = []
                        for _, machine in ss.df_machines.iterrows():
                            maint = machine.get('Maintenance_Window')
                            if maint:
                                windows = []
                                if isinstance(maint, dict) and maint:
                                    windows = [maint]
                                elif isinstance(maint, list):
                                    windows = [w for w in maint if isinstance(w, dict) and w]
                                
                                if windows:
                                    machines_with_breakdowns.append(machine['Machine_ID'])
                                    for window in windows:
                                        total_breakdown_hours += window.get('duration', 0) / 60
                        
                        breakdown_info = f"\nBreakdowns/Maintenance: {len(machines_with_breakdowns)} machine(s), {total_breakdown_hours:.1f} total hours" if machines_with_breakdowns else "\nBreakdowns/Maintenance: None"
                        
                        context = f"""
                        DATASET: {len(df_ops)} ops, {df_ops['Job_ID'].nunique()} jobs, {len(ss.df_machines)} machines
                        Release: {df_ops['Release_Day'].min()}-{df_ops['Release_Day'].max()} days
                        Due: {df_ops['Due_Day'].min()}-{df_ops['Due_Day'].max()} days
                        Outsourced: {len(df_ops[df_ops['Assignment_Type'] == 'OUTSOURCE'])}/{len(df_ops)} ({len(df_ops[df_ops['Assignment_Type'] == 'OUTSOURCE'])/len(df_ops)*100:.1f}%)
                        Lead time avg: {(df_ops['Due_Day'] - df_ops['Release_Day']).mean():.1f} days{breakdown_info}
                        
                        Provide: 1) Quality score 1-10, 2) Critical issues, 3) Optimization tips
                        """
                        insights = generate_ai_insights("Analyze CNC dataset quality", context_data=context)
                        if insights:
                            st.markdown(insights)
                            st.caption("üí° Powered by Gemini AI")
                        else:
                            st.info("üí° Install `google-generativeai` and add API key to .env")
            
            with tab2:
                st.info("Run 'Compute All Algorithms' first, then AI analysis will appear in results below")
    
    with col2:
        with st.expander("‚öñÔ∏è Priority Weights", expanded=False):
            w_makespan = st.slider("Makespan", 0.0, 5.0, 1.0, 0.1, key='w_makespan')
            w_tardiness = st.slider("Tardiness", 0.0, 5.0, 1.0, 0.1, key='w_tardiness')
            w_ontime = st.slider("On-Time %", 0.0, 5.0, 1.0, 0.1, key='w_ontime')
            w_util = st.slider("Utilization", 0.0, 5.0, 1.0, 0.1, key='w_util')
            w_cost = st.slider("Cost", 0.0, 5.0, 1.0, 0.1, key='w_cost')
            weights_sum = w_makespan + w_tardiness + w_ontime + w_util + w_cost

    st.divider()
    
    # === COMPARISON RESULTS ===

    # ‚úÖ Automatically recompute metrics when dataset changes
    recalc_flag = st.session_state.get('recalculate_all_heuristics', False)
    if recalc_flag:
        st.warning("‚ö†Ô∏è DETECTED: Dataset changed ‚Äî recalculating comparison metrics...")
        with st.spinner("üîÑ Recalculating all 4 heuristics..."):
            try:
                metrics = []
                heuristics = ['SPT', 'EDD', 'CR', 'PRIORITY']
                for i, heur in enumerate(heuristics):
                    st.write(f"  {i+1}/4: Computing {heur} metrics...")
                    schedule_key = f'schedule_{heur.lower()}'
                    schedule = getattr(ss, schedule_key, None)
                    if schedule is not None:
                        metric = calculate_metrics(schedule.copy(), ss.base_df_ops.copy(), heur)
                        metrics.append(metric)
                        st.write(f"    ‚úÖ {heur}: Makespan={metric.get('Makespan_Days', 'N/A'):.2f}")
                    else:
                        st.warning(f"    ‚ùå {heur}: Schedule not found!")
                ss.df_metrics = pd.DataFrame(metrics)
                st.session_state.recalculate_all_heuristics = False
                st.success("‚úÖ Comparison table UPDATED!")
            except Exception as e:
                st.error(f"‚ùå Error recalculating metrics: {str(e)}")
                import traceback
                st.write(traceback.format_exc())
                ss.recalculate_all_heuristics = False

    # ‚úÖ Display comparison results
    if hasattr(ss, 'df_metrics') and ss.df_metrics is not None and len(ss.df_metrics) > 0:
        try:
            # Calculate composite scores
            dfm = ss.df_metrics.copy()
            for col in ['Makespan_Days', 'Total_Tardiness_Days', 'On_Time_%', 'Machine_Utilization_%', 'Total_Cost_$']:
                if col not in dfm.columns:
                    dfm[col] = 0.0

            # Normalize metrics (0-1 scale, higher is better)
            def norm(s):
                mn, mx = s.min(), s.max()
                return pd.Series([0.5] * len(s), index=s.index) if mx == mn else (s - mn) / (mx - mn)

            n_makespan = 1.0 - norm(dfm['Makespan_Days'])
            n_tardiness = 1.0 - norm(dfm['Total_Tardiness_Days'])
            n_ontime = norm(dfm['On_Time_%'])
            n_util = norm(dfm['Machine_Utilization_%'])
            n_cost = 1.0 - norm(dfm['Total_Cost_$'])

            # Weighted composite score
            total_weight = max(1e-9, (w_makespan + w_tardiness + w_ontime + w_util + w_cost))
            dfm['Score'] = (w_makespan * n_makespan + w_tardiness * n_tardiness + 
                           w_ontime * n_ontime + w_util * n_util + w_cost * n_cost) / total_weight

            # Display ranked table
            dfm_display = dfm.sort_values('Score', ascending=False).reset_index(drop=True)
            st.dataframe(dfm_display.style.highlight_max(subset=['Score'], color='lightgreen'), use_container_width=True)

            # Recommendation
            best = dfm_display.iloc[0]
            st.success(f"üèÜ **Recommended: {best['Heuristic']}** (Score: {best['Score']:.3f}) ‚Äî Makespan: {best['Makespan_Days']:.1f}d, Tardiness: {best['Total_Tardiness_Days']:.1f}d, On-Time: {best['On_Time_%']:.1f}%")

            # AI Analysis (optional)
            with st.expander("ü§ñ Why This Algorithm?", expanded=False):
                if os.getenv('GEMINI_API_KEY') and st.checkbox("Enable AI Analysis", value=False, key='enable_gemini'):
                    with st.spinner("Generating insights..."):
                        try:
                            import google.generativeai as genai
                            model = get_gemini_model()
                            prompt = f"""Explain why {best['Heuristic']} (score {best['Score']:.3f}) is best for this CNC scheduling scenario:
                            - Makespan: {best['Makespan_Days']:.1f}d, Tardiness: {best['Total_Tardiness_Days']:.1f}d, On-Time: {best['On_Time_%']:.1f}%
                            - User priorities: Makespan={w_makespan}, Tardiness={w_tardiness}, On-Time={w_ontime}, Util={w_util}, Cost={w_cost}
                            
                            Provide: 1) Why it's optimal, 2) Business impact, 3) Trade-offs vs 2nd best. Be concise (3-4 sentences)."""
                            st.markdown(model.generate_content(prompt).text)
                        except Exception as e:
                            st.warning(f"AI unavailable: {e}")
                else:
                    st.info("üí° **Quick Guide:**\n- **SPT**: Fast completion, good for throughput\n- **EDD**: Minimizes tardiness, best for deadlines\n- **CR**: Balances urgency and workload\n- **PRIORITY**: Follows business priorities")

        except Exception as e:
            st.error(f"Error computing recommendation: {e}")
            
    else:
        st.info("üìä No comparison data yet. Click 'üß™ Compute All Heuristics' in the sidebar to generate comparison results.")

# ---------------------------
# Outsourcing & Machine Utilization Analysis
# ---------------------------
def draw_outsourcing_analysis(ss):
    """
    Analyzes why operations are being outsourced and provides AI-powered recommendations
    to reduce outsourcing and improve machine utilization.
    """
    st.header("üè≠ Outsourcing & Machine Utilization Analysis")
    
    # Get outsourcing statistics
    total_ops = len(ss.df_ops)
    outsourced_ops = ss.df_ops[ss.df_ops['Assignment_Type'] == 'OUTSOURCE']
    inhouse_ops = ss.df_ops[ss.df_ops['Assignment_Type'] == 'IN_HOUSE']
    outsource_count = len(outsourced_ops)
    outsource_pct = (outsource_count / total_ops) * 100 if total_ops > 0 else 0
    
    # Display summary metrics
    col1, col2, col3 = st.columns(3)
    col1.metric("Total Operations", total_ops)
    col2.metric("Outsourced", f"{outsource_count} ({outsource_pct:.1f}%)")
    col3.metric("In-House", f"{len(inhouse_ops)} ({100-outsource_pct:.1f}%)")
    
    if outsource_pct > 50:
        st.error(f"‚ö†Ô∏è **HIGH OUTSOURCING ALERT**: {outsource_pct:.1f}% of operations are outsourced!")
    elif outsource_pct > 30:
        st.warning(f"‚ö†Ô∏è **MODERATE OUTSOURCING**: {outsource_pct:.1f}% of operations are outsourced")
    else:
        st.success(f"‚úÖ **HEALTHY OUTSOURCING LEVEL**: {outsource_pct:.1f}% of operations are outsourced")
    
    # Machine utilization analysis
    st.subheader("üîß Machine Utilization Breakdown")
    
    if not ss.current_schedule.empty:
        machine_stats = []
        for machine_id in ss.df_machines['Machine_ID'].unique():
            machine_ops = ss.current_schedule[ss.current_schedule['Machine_ID'] == machine_id]
            job_count = len(machine_ops)
            
            # Calculate operation types
            op_types = []
            for op_id in machine_ops['Operation_ID'].unique():
                op_data = ss.df_ops[ss.df_ops['Operation_ID'] == op_id]
                if not op_data.empty:
                    op_types.append(op_data.iloc[0].get('Op_Type', 'N/A'))
            
            unique_op_types = list(set(op_types))
            
            machine_stats.append({
                'Machine': machine_id,
                'Jobs': job_count,
                'Operation Types': ', '.join(unique_op_types) if unique_op_types else 'None',
                'Total Time (hrs)': (machine_ops['Setup_Time'].sum() + 
                                     machine_ops['Proc_Time'].sum() + 
                                     machine_ops['Transfer_Time'].sum()) / 60 if job_count > 0 else 0
            })
        
        machine_df = pd.DataFrame(machine_stats).sort_values('Jobs', ascending=False)
        st.dataframe(machine_df, use_container_width=True)
        
        # Highlight low-utilization machines
        low_util_machines = machine_df[machine_df['Jobs'] <= 1]
        if not low_util_machines.empty:
            st.warning(f"‚ö†Ô∏è **Underutilized Machines**: {', '.join(low_util_machines['Machine'].tolist())} have very few jobs assigned")
    
    # Outsourcing reasons breakdown
    st.subheader("üìã Why Are Operations Outsourced?")
    
    if outsource_count > 0:
        # Group by operation type
        outsource_by_type = outsourced_ops.groupby('Op_Type').size().reset_index(name='Count')
        outsource_by_type['Percentage'] = (outsource_by_type['Count'] / outsource_count * 100).round(1)
        
        col1, col2 = st.columns(2)
        with col1:
            st.write("**By Operation Type:**")
            st.dataframe(outsource_by_type, use_container_width=True)
        
        with col2:
            st.write("**By Priority:**")
            outsource_by_priority = outsourced_ops.groupby('Priority').size().reset_index(name='Count')
            st.dataframe(outsource_by_priority, use_container_width=True)
        
        # Cost analysis
        if 'Outsource_Cost' in outsourced_ops.columns:
            total_outsource_cost = outsourced_ops['Outsource_Cost'].sum()
            avg_outsource_cost = outsourced_ops['Outsource_Cost'].mean()
            
            st.metric("Total Outsourcing Cost", f"${total_outsource_cost:,.2f}")
            st.metric("Average Cost per Operation", f"${avg_outsource_cost:,.2f}")
    
    # AI-Powered Analysis
    st.divider()
    with st.expander("ü§ñ AI-Powered Outsourcing Analysis", expanded=True):
        if st.button("Generate Detailed Analysis", key='outsource_ai_analysis'):
            with st.spinner("üß† Analyzing outsourcing patterns..."):
                # Prepare context
                machine_capacity = {}
                for machine_id in ss.df_machines['Machine_ID'].unique():
                    eligible_ops = []
                    for op_type in ['MILLING', 'TURNING', 'GRINDING', 'DRILLING']:
                        if machine_id in get_eligible_machines(op_type):
                            eligible_ops.append(op_type)
                    machine_capacity[machine_id] = eligible_ops
                
                context = f"""
                CNC MANUFACTURING OUTSOURCING ANALYSIS:
                
                CURRENT STATE:
                - Total Operations: {total_ops}
                - Outsourced: {outsource_count} ({outsource_pct:.1f}%)
                - In-House: {len(inhouse_ops)} ({100-outsource_pct:.1f}%)
                - Total Outsourcing Cost: ${outsourced_ops['Outsource_Cost'].sum() if 'Outsource_Cost' in outsourced_ops.columns else 0:,.2f}
                
                MACHINE CAPABILITIES:
                {chr(10).join([f"- {m}: {', '.join(ops)}" for m, ops in machine_capacity.items()])}
                
                OUTSOURCED OPERATION TYPES:
                {chr(10).join([f"- {row['Op_Type']}: {row['Count']} ops ({row['Percentage']:.1f}%)" for _, row in outsource_by_type.iterrows()]) if outsource_count > 0 else "None"}
                
                DECISION CRITERIA:
                - Cost Threshold: 90% (outsource if vendor cost < 90% of in-house cost)
                - Deadline Priority: Operations that can't meet deadlines are outsourced
                - Current Heuristic: {ss.current_heuristic or 'None selected'}
                
                ANALYSIS REQUIRED:
                1. ROOT CAUSE ANALYSIS: Why is outsourcing so high? Is it:
                   a) Machine capacity limitations?
                   b) Operation type mismatch (ops requiring machines we don't have)?
                   c) Cost advantages from vendors?
                   d) Scheduling inefficiency?
                
                2. MACHINE UTILIZATION ISSUE: Why do some machines (like M6, M9) have very few jobs?
                   - Is it dataset composition (few TURNING/GRINDING operations)?
                   - Are these machines underutilized due to poor scheduling?
                
                3. ACTIONABLE RECOMMENDATIONS:
                   - Should we adjust cost threshold? (currently 0.9)
                   - Do we need more machines or different machine types?
                   - Can scheduling be improved to reduce outsourcing?
                   - What's the optimal outsourcing percentage for this operation mix?
                
                4. QUICK WINS: 3 immediate actions to reduce outsourcing costs while maintaining delivery performance.
                
                Provide detailed, specific, and actionable insights for a manufacturing operations manager.
                """
                
                insights = generate_ai_insights(
                    "Provide comprehensive outsourcing and machine utilization analysis with specific recommendations.",
                    context_data=context
                )
                
                if insights:
                    st.markdown(insights)
                    st.caption("üí° Analysis powered by Google Gemini AI")
                else:
                    st.warning("‚ö†Ô∏è AI analysis requires `google-generativeai` package and valid GEMINI_API_KEY in .env file")
                    
                    # Provide manual analysis
                    st.info("""
                    **Manual Analysis (AI unavailable):**
                    
                    **Common Reasons for High Outsourcing:**
                    1. **Dataset Composition**: Your dataset may have many operations requiring machine types you don't have
                    2. **Cost Threshold Too Aggressive**: Current 0.9 threshold means outsourcing if vendor is <90% of in-house cost
                    3. **Machine Type Mismatch**: 
                       - M1, M3, M4: Handle MILLING & DRILLING
                       - M6, M9: Handle TURNING & GRINDING
                       - If your jobs are mostly MILLING/DRILLING, M6/M9 will be underutilized
                    
                    **Recommendations:**
                    1. Check dataset: Run query to see operation type distribution
                    2. Adjust cost threshold in sidebar (Advanced Settings ‚Üí Outsourcing Policy)
                    3. Consider adding machines for underrepresented operation types
                    4. Review vendor costs - some vendors may be too competitive
                    """)

def handle_error(e):
    st.error(f"‚ùå ERROR: An error occurred during execution")
    st.error(f"Error Type: {type(e).__name__}")
    st.error(f"Error Message: {str(e)}")
    import traceback
    st.code(traceback.format_exc())

# ---------------------------
# Diagnostic utility: compute realistic utilization metrics
# ---------------------------
def display_utilization_diagnostics(ss):
    """
    Diagnostic panel showing why utilization is low and what realistic targets are.
    """
    try:
        st.markdown("### üîç Utilization Diagnostic Report")
        
        if not hasattr(ss, 'base_df_ops') or not hasattr(ss, 'base_df_machines'):
            st.warning("Data not loaded yet. Initialize the app first.")
            return
        
        df_ops = ss.base_df_ops.copy()
        df_machines = ss.base_df_machines.copy()
        
        # Calculate total work content (productive time)
        total_setup = df_ops['Setup_Time'].sum()
        total_proc = df_ops['Total_Proc_Min'].sum()
        total_transfer = df_ops.get('Transfer_Min', pd.Series([0]*len(df_ops))).sum()
        total_productive_min = total_setup + total_proc + total_transfer
        total_productive_days = total_productive_min / 480
        
        # Count operations by assignment
        inhouse_ops = len(df_ops[df_ops['Assignment_Type'] == 'IN_HOUSE'])
        outsourced_ops = len(df_ops[df_ops['Assignment_Type'] == 'OUTSOURCE'])
        total_ops = len(df_ops)
        
        # Machine count
        num_machines = len(df_machines)
        
        # Calculate planning horizon
        min_release = df_ops['Release_Day'].min()
        max_due = df_ops['Due_Day'].max()
        horizon_days = max_due - min_release
        
        # Theoretical utilization (if work was packed perfectly)
        if horizon_days > 0 and num_machines > 0:
            theoretical_util = (total_productive_days / (num_machines * horizon_days)) * 100
        else:
            theoretical_util = 0
        
        # Display metrics
        col1, col2, col3 = st.columns(3)
        col1.metric("Total Operations", total_ops)
        col1.metric("In-House Ops", inhouse_ops)
        col1.metric("Outsourced Ops", outsourced_ops)
        
        col2.metric("Total Work Content", f"{total_productive_days:.1f} days")
        col2.metric("Machine Count", num_machines)
        col2.metric("Planning Horizon", f"{horizon_days:.1f} days")
        
        col3.metric("Theoretical Max Utilization", f"{theoretical_util:.1f}%")
        col3.metric("Realistic Target", f"{theoretical_util * 0.7:.1f}%")
        
        st.divider()
        
        st.markdown("### üìã Interpretation")
        
        if theoretical_util < 10:
            st.error(f"""
            ‚ö†Ô∏è **Very Low Utilization ({theoretical_util:.1f}%)**
            
            **Why this is happening:**
            - You have {num_machines} machines available for {horizon_days:.1f} days = {num_machines * horizon_days:.1f} machine-days of capacity.
            - But only {total_productive_days:.1f} days of actual work content.
            - Even if perfectly packed, machines would be idle {100-theoretical_util:.1f}% of the time.
            
            **Recommended fixes:**
            1. **Reduce machine count** to 2-3 machines (or use full dataset with more jobs).
            2. **Reduce outsourcing** (adjust threshold in sidebar to keep more work in-house).
            3. **Fix Release/Due dates** in your CSV (currently all Release_Day=33 but Due_Day=5-32).
            4. **Load full dataset** (change SAMPLE_SIZE to None in code).
            """)
        elif theoretical_util < 30:
            st.warning(f"""
            ‚ö†Ô∏è **Low Utilization ({theoretical_util:.1f}%)**
            
            Your current workload can't fully utilize {num_machines} machines over {horizon_days:.1f} days.
            
            **Options:**
            - Reduce machines to {int(num_machines * theoretical_util / 60) + 1}-{int(num_machines * theoretical_util / 50) + 1} to reach 50-60% utilization.
            - Add more jobs or reduce outsourcing to increase in-house work content.
            """)
        elif theoretical_util < 70:
            st.success(f"""
            ‚úÖ **Reasonable Utilization Range ({theoretical_util:.1f}%)**
            
            Your workload is balanced for the number of machines. Target realistic utilization: {theoretical_util * 0.7:.1f}%.
            """)
        else:
            st.info(f"""
            üìä **High Capacity Utilization ({theoretical_util:.1f}%)**
            
            Machines will be busy most of the time. Monitor for:
            - Potential bottlenecks or delays.
            - Need for overtime or additional capacity.
            """)
        
        # Show breakdown by operation type
        st.markdown("### üìä Work Breakdown by Operation Type")
        op_breakdown = df_ops.groupby('Op_Type').agg({
            'Operation_ID': 'count',
            'Total_Proc_Min': 'sum',
            'Setup_Time': 'sum'
        }).rename(columns={'Operation_ID': 'Count', 'Total_Proc_Min': 'Total_Proc_Min', 'Setup_Time': 'Total_Setup_Min'})
        op_breakdown['Total_Time_Days'] = (op_breakdown['Total_Proc_Min'] + op_breakdown['Total_Setup_Min']) / 480
        st.dataframe(op_breakdown, use_container_width=True)
        
    except Exception as e:
        st.error(f"Error computing diagnostics: {e}")
        import traceback
        st.code(traceback.format_exc())

# ---------------------------
# MAIN
# ---------------------------
# Configuration: Set to None to load full dataset (200 jobs), or specify number of jobs to sample
SAMPLE_SIZE = None  # Load ALL 200 jobs for realistic utilization

if 'recalculate_all_heuristics' not in st.session_state:
    st.session_state.recalculate_all_heuristics = False
if 'force_metric_refresh' not in st.session_state:
    st.session_state.force_metric_refresh = False

def initialize_app(ss):
    dbg("üîß DEBUG: System not initialized, starting initialization...")
    dbg("üîÑ DEBUG: Clearing old cached data...")
    st.cache_data.clear()

    with st.spinner("Loading and preprocessing data..."):
        df_ops, df_machines, df_effective, df_penalties, df_vendors = load_all_data(sample_size=SAMPLE_SIZE)
        dbg("‚úÖ DEBUG: Data loaded successfully")
        dbg(f"  - Jobs: {df_ops['Job_ID'].nunique()}, Operations: {len(df_ops)}")
        dbg(f"  - Machines: {len(df_machines)}")
        dbg(f"  - Effective times: {len(df_effective)}")

        ss.base_df_ops = df_ops
        ss.base_df_machines = df_machines
        ss.base_df_effective = df_effective
        ss.base_df_penalties = df_penalties
        ss.base_df_vendors = df_vendors
        ss.machine_order = sorted(ss.base_df_machines['Machine_ID'].unique(), key=lambda x: int(x[1:]) if x[1:].isdigit() else x)

        dbg("‚úÖ DEBUG: Base data stored in session state")

    # DO NOT run SPT automatically ‚Äî user will compute heuristics explicitly
    ss.schedule_spt = None
    ss.schedule_edd = None
    ss.schedule_cr = None
    ss.schedule_priority = None

    ss.df_ops = ss.base_df_ops.copy()
    ss.df_machines = ss.base_df_machines.copy()
    ss.current_schedule = pd.DataFrame()
    ss.cost_threshold = 0.9
    ss.initialized = True
    if "current_heuristic" not in ss:
        ss.current_heuristic = None
    if "last_applied_heuristic" not in ss:
        ss.last_applied_heuristic = None



    st.info("üîé Raw dataset loaded. Click 'Compute All Heuristics' in the sidebar to run scheduling on current data.")
    safe_toast("System initialized (raw data loaded).", icon="‚úÖ")

def main():
    st.set_page_config(page_title="CNC Scheduling", page_icon="üè≠", layout="wide")
    st.title("üè≠ CNC Job Scheduling System")
    
    if 'cache_version' not in st.session_state or st.session_state.cache_version < 2:
        st.session_state.cache_version = 2

    ss = st.session_state
    if "current_page" not in ss:
        ss.current_page = "comparison"

    try:
        if 'initialized' not in ss:
            initialize_app(ss)
        if 'schedule_update_key' not in ss:
            ss.schedule_update_key = str(time.time())

        # ============ SIMPLIFIED SIDEBAR ============
        with st.sidebar:
            st.title("‚öôÔ∏è Controls")
            
            # Page Navigation
            page = st.radio(
                "ÔøΩ Navigation",
                ["üìä Compare Algorithms", "üîç View Schedule", "üè≠ Outsourcing Analysis", "üìú Activity Log"],
            index=0 if ss.current_page == "comparison" else (2 if ss.current_page == "outsourcing" else (3 if ss.current_page == "activity_log" else 1))
)
            if "Compare" in page:
                ss.current_page = "comparison"
            elif "Outsourcing" in page:
                ss.current_page = "outsourcing"
            elif "Activity Log" in page:
                ss.current_page = "activity_log"
            else:
                ss.current_page = "heuristic_view"
            
            st.divider()
            
            # Section 1: Compute & Apply
            with st.expander("üéØ **1. Compute & Apply**", expanded=True):
                if st.button("üß™ Compute All Algorithms", use_container_width=True):
                    compute_all_heuristics_and_metrics(ss, show_progress=True)
                    st.rerun()
                
                heuristic_options = ('SPT', 'EDD', 'CR', 'PRIORITY')
                current_h = ss.get("current_heuristic")
                current_index = heuristic_options.index(current_h) if current_h in heuristic_options else 0
                
                apply_choice = st.selectbox(
                    "Select algorithm:", 
                    heuristic_options,
                    index=current_index,
                    help="Choose which algorithm to apply to your schedule"
                )
                
                # Check if any schedules have been computed
                schedules_computed = (
                    hasattr(ss, 'schedule_spt') and ss.schedule_spt is not None and not ss.schedule_spt.empty or
                    hasattr(ss, 'schedule_edd') and ss.schedule_edd is not None and not ss.schedule_edd.empty or
                    hasattr(ss, 'schedule_cr') and ss.schedule_cr is not None and not ss.schedule_cr.empty or
                    hasattr(ss, 'schedule_priority') and ss.schedule_priority is not None and not ss.schedule_priority.empty
                )
                
                if st.button("‚úÖ Apply Algorithm", key='apply_heur', use_container_width=True, disabled=not schedules_computed):
                    ok = apply_heuristic_to_dataset(ss, apply_choice)
                    if ok:
                        st.rerun()
                
                if not schedules_computed:
                    st.caption("‚ö†Ô∏è Click 'Compute All Algorithms' first")
            
            # Section 2: Manage Jobs
            with st.expander("üìã **2. Manage Jobs**", expanded=False):
                draw_live_job_scheduler(ss)
                st.divider()
                draw_job_deleter(ss)
            
            # Section 3: Advanced Settings
            with st.expander("‚öôÔ∏è **3. Advanced Settings**", expanded=False):
                draw_breakdown_simulator(ss)
                st.divider()
                draw_priority_manager(ss)
                st.divider()
                draw_outsourcing_policy(ss)
            
            # Section 4: System
            st.divider()
            col1, col2 = st.columns(2)
            with col1:
                if st.button("üîÑ Reset", use_container_width=True):
                    st.cache_data.clear()
                    st.cache_resource.clear()
                    for key in list(ss.keys()):
                        del ss[key]
                    st.rerun()
            
            with col2:
                csv_data = export_schedule(ss.current_schedule)
                safe_name = (ss.current_heuristic or "none").lower()
                st.download_button(
                    label="üíæ Export",
                    data=csv_data,
                    file_name=f"schedule_{safe_name}.csv",
                    mime='text/csv',
                    use_container_width=True
                )

        # ============ MAIN CONTENT ============
        if ss.current_page == "comparison":
            st.header("üìä Algorithm Comparison")
            draw_comparison_tab(ss)
        elif ss.current_page == "outsourcing":
            draw_outsourcing_analysis(ss)
        elif ss.current_page == "activity_log":
            draw_activity_log(ss)
        else:
            if ss.current_heuristic:
                st.header(f"üìà {ss.current_heuristic} Schedule")
                draw_kpi_dashboard(ss)
                
                tab1, tab2 = st.tabs(["üìà Gantt Chart", "üìã Operations"])
                with tab1:
                    draw_gantt_tab(ss)
                with tab2:
                    draw_operation_status_tab(ss)
            else:
                st.warning("‚ö†Ô∏è No algorithm applied. Go to 'Compare Algorithms' to select one.")

    except Exception as e:
        handle_error(e)

if __name__ == "__main__":
    main()
